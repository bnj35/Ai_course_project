{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54b634ad",
   "metadata": {},
   "source": [
    "# Artificial Intelligence (AI) & Machine Learning (ML): Project 3\n",
    "\n",
    "###### G3: ADEN Abdillahi, AUGER Benjamin, LERNER Shaun, MEES Thomas, SALINAS David\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The pharmaceutical company *HumanForYou* wishes to determine which factors are most contributing to their 15% turnover rate, then provide suggestions for which areas deserve the most focus of effort to reduce employee churn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11f6fb6",
   "metadata": {},
   "source": [
    "## Import & Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "77d4fcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import f_classif, mutual_info_classif\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve, auc, precision_recall_curve\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "target_col = \"Attrition\"\n",
    "random_state = 42 # np.random.random()\n",
    "\n",
    "folder_path = \"data\"\n",
    "time_folder_path = \"in_out_time\"\n",
    "in_time_file_name = \"in_time.csv\"\n",
    "out_time_file_name = \"out_time.csv\"\n",
    "employee_file_name = \"employee_survey_data.csv\"\n",
    "general_file_name = \"general_data.csv\"\n",
    "manager_file_name = \"manager_survey_data.csv\"\n",
    "\n",
    "employee_data = pd.read_csv(os.path.join(folder_path, employee_file_name))\n",
    "general_data = pd.read_csv(os.path.join(folder_path, general_file_name))\n",
    "manager_data = pd.read_csv(os.path.join(folder_path, manager_file_name))\n",
    "in_time_data = pd.read_csv(os.path.join(folder_path, time_folder_path, in_time_file_name))\n",
    "out_time_data = pd.read_csv(os.path.join(folder_path, time_folder_path, out_time_file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787cebd2",
   "metadata": {},
   "source": [
    "\n",
    "This block initializes the analysis environment by importing essential libraries for:\n",
    "* **Data Manipulation:** `pandas`, `numpy`\n",
    "* **Visualization:** `matplotlib`\n",
    "* **Machine Learning:** `sklearn` - Random Forest, metrics, preprocessing, `imblearn` SMOTE (Synthetic Minority Over-sampling Technique) for balancing\n",
    "\n",
    "It also configures global parameters, such as the `random_state` for reproducibility and the `target_col` (\"Attrition\"). Finally, it loads the five distinct CSV datasets provided by the HR department (General Data, Employee Survey, Manager Survey, and In/Out Time logs) into Pandas DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5fe7a3",
   "metadata": {},
   "source": [
    "### In/Out Time Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "12c23a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge in_time and out_time data on the first column (actually EmployeeID, so rename)\n",
    "in_time_data.rename(columns={in_time_data.columns[0]: \"EmployeeID\"}, inplace=True)\n",
    "out_time_data.rename(columns={out_time_data.columns[0]: \"EmployeeID\"}, inplace=True)\n",
    "\n",
    "# confirm days consistency\n",
    "in_time_days = set(in_time_data.columns[1:])\n",
    "out_time_days = set(out_time_data.columns[1:])\n",
    "missing_in_out = in_time_days.difference(out_time_days)\n",
    "if len(missing_in_out) > 0:\n",
    "    print(f\"Days missing in either in_time or out_time data: {missing_in_out}\")\n",
    "\n",
    "# go through each column to check empty cells present only in one of the datasets\n",
    "for day in in_time_days.intersection(out_time_days):\n",
    "    in_time_empty = set(in_time_data.index[in_time_data[day].isnull()])\n",
    "    out_time_empty = set(out_time_data.index[out_time_data[day].isnull()])\n",
    "    missing_in_out_rows = in_time_empty.symmetric_difference(out_time_empty)\n",
    "    if missing_in_out_rows:\n",
    "        print(f\"Day {day} has missing entries in either in_time or out_time data at rows: {missing_in_out_rows}\")\n",
    "\n",
    "# convert all columns except the first one to datetime\n",
    "for col in in_time_data.columns[1:]:\n",
    "    in_time_data[col] = pd.to_datetime(in_time_data[col], format=\"%Y-%m-%d %H:%M:%S\", errors=\"coerce\")\n",
    "for col in out_time_data.columns[1:]:\n",
    "    out_time_data[col] = pd.to_datetime(out_time_data[col], format=\"%Y-%m-%d %H:%M:%S\", errors=\"coerce\")\n",
    "\n",
    "def remove_col_depending_on_distinct_values(df, start_threshold=0, end_threshold=0):\n",
    "    cols_to_remove = []\n",
    "    for col in df.columns:\n",
    "        if start_threshold <= df[col].nunique() <= end_threshold:\n",
    "            cols_to_remove.append(col)\n",
    "    df.drop(columns=cols_to_remove, inplace=True)\n",
    "    return df\n",
    "\n",
    "# combine based on EmployeeID\n",
    "time_data = pd.merge(in_time_data, out_time_data, on=\"EmployeeID\", suffixes=(\"_in\", \"_out\"))\n",
    "\n",
    "# create a new column for duration time in hours\n",
    "hours_columns = {}\n",
    "day_of_week_columns = {}\n",
    "for day in in_time_days.intersection(out_time_days):\n",
    "    hours_columns[f\"{day}_hours\"] = (time_data[f\"{day}_out\"] - time_data[f\"{day}_in\"]).dt.total_seconds() / 3600.0\n",
    "    day_of_week_columns[f\"{day}_day_of_week\"] = time_data[f\"{day}_in\"].dt.dayofweek\n",
    "\n",
    "# duration_hours\n",
    "time_data = pd.concat([time_data, pd.DataFrame(hours_columns, index=time_data.index)], axis=1)\n",
    "time_data = pd.concat([time_data, pd.DataFrame(day_of_week_columns, index=time_data.index)], axis=1)\n",
    "time_data[\"duration_hours\"] = time_data[list(hours_columns.keys())].sum(axis=1)\n",
    "\n",
    "# aggregate by day of week\n",
    "day_of_week_counts = {}\n",
    "day_of_week_avg_hours = {}\n",
    "for i in range(7): # 0=Monday through 6=Sunday\n",
    "    count_cols = [col for col in time_data.columns if col.endswith(\"_day_of_week\")]\n",
    "    day_of_week_counts[f\"worked_on_day_{i}\"] = sum(\n",
    "        (time_data[col] == i).astype(int) for col in count_cols\n",
    "    )\n",
    "    \n",
    "    # avg hrs per day of week\n",
    "    total_hours = 0\n",
    "    for day in in_time_days.intersection(out_time_days):\n",
    "        day_col = f\"{day}_day_of_week\"\n",
    "        hours_col = f\"{day}_hours\"\n",
    "        if day_col in time_data.columns and hours_col in time_data.columns:\n",
    "            # only sum hours where the day of week matches\n",
    "            mask = time_data[day_col] == i\n",
    "            total_hours += time_data[hours_col].where(mask, 0)\n",
    "    day_of_week_avg_hours[f\"avg_hours_day_{i}\"] = total_hours / day_of_week_counts[f\"worked_on_day_{i}\"].replace(0, 1)\n",
    "\n",
    "time_data = pd.concat([time_data, pd.DataFrame(day_of_week_counts, index=time_data.index)], axis=1)\n",
    "time_data = pd.concat([time_data, pd.DataFrame(day_of_week_avg_hours, index=time_data.index)], axis=1)\n",
    "\n",
    "# remove columns with 0 distinct values\n",
    "remove_col_depending_on_distinct_values(time_data)\n",
    "\n",
    "# keep only columns: EmployeeID, duration_hours, worked_on_day_*, avg_hours_day_*\n",
    "cols_to_keep = [\"EmployeeID\", \"duration_hours\"] + [col for col in time_data.columns if col.startswith(\"worked_on_day_\") or col.startswith(\"avg_hours_day_\")]\n",
    "time_data = time_data[cols_to_keep]\n",
    "time_data = pd.concat([time_data, pd.DataFrame(hours_columns, index=time_data.index)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae868b3",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This block focuses on transforming raw arrival and departure logs into useful behavioral metrics:\n",
    "\n",
    "* **Data Cleaning & Validation:** Renames the primary key to `EmployeeID`, checks for inconsistencies between the in_time and out_time files (missing days or mismatched rows), and converts all timestamp strings into datetime objects.\n",
    "* **Daily Calculations:** Merges the datasets and calculates the daily duration of work (Departure Time - Arrival Time) and identifies the specific day of the week for each date.\n",
    "* **Aggregation:** Generates summary statistics per employee to capture work habits:\n",
    "    * `duration_hours`: Total hours worked over the recorded period.\n",
    "    * `worked_on_day_X`: Frequency of working on specific weekdays (0=Mon to 6=Sun).\n",
    "    * `avg_hours_day_X`: Average shift length for each specific weekday.\n",
    "* **Final Filtering:** Discards the raw daily timestamp columns, retaining only the `EmployeeID` and the newly calculated aggregated features for the final dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0154753",
   "metadata": {},
   "source": [
    "## Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "fb7a5852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Data Merging\n",
      "======================================================================\n",
      "Total dataset size: 4410 samples\n",
      "Features: 302 columns\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Data Merging\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Merge employee and manager data first\n",
    "employee_manager_data = pd.merge(employee_data, manager_data, on=\"EmployeeID\", suffixes=(\"_emp\", \"_mgr\"))\n",
    "# Merge all datasets into a final dataset on EmployeeID\n",
    "raw_dataset = pd.merge(general_data, employee_manager_data, on=\"EmployeeID\")\n",
    "raw_dataset = pd.merge(raw_dataset, time_data, on=\"EmployeeID\")\n",
    "\n",
    "# Drop unethical columns BEFORE split\n",
    "raw_dataset.drop(columns=[\"MaritalStatus\", \"Gender\", \"Age\"], inplace=True)\n",
    "\n",
    "print(f\"Total dataset size: {len(raw_dataset)} samples\")\n",
    "print(f\"Features: {len(raw_dataset.columns)} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2c622c",
   "metadata": {},
   "source": [
    "\n",
    "This block merges all separate data sources into a single master dataset for analysis and applies ethical constraints:\n",
    "\n",
    "* **Integration:** Joins `general_data`, `employee_survey_data`, `manager_survey_data`, and the processed `time_data` into one unified DataFrame (`raw_dataset`) using `EmployeeID` as the unique key.\n",
    "* **Ethical Feature Selection:** Proactively removes sensitive demographic attributes—`MaritalStatus`, `Gender`, and `Age`—before training. This ensures the predictive model focuses on performance and work environment factors rather than personal characteristics, mitigating the risk of bias or discrimination.\n",
    "* **Verification:** Prints the final row and column counts to confirm the data integration process.\n",
    "\n",
    "### Ethics\n",
    "We've already needed to consider ethics here for which columns should remain before proceeding with the training/testing split, so we've dedicated a section for ethics below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01394f6a",
   "metadata": {},
   "source": [
    "## Feature Engineering & Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b8f51f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Feature Engineering & Split\n",
      "======================================================================\n",
      "Features after engineering: 315 columns\n",
      "Advanced features created successfully\n",
      "Train set: 3528 samples\n",
      "Test set: 882 samples\n",
      "Train class distribution: {'No': 2959, 'Yes': 569}\n",
      "Test class distribution: {'No': 740, 'Yes': 142}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Feature Engineering & Split\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def create_advanced_features(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    if 'YearsSinceLastPromotion' in df.columns:\n",
    "        df['YearsWithoutPromotion'] = df['YearsSinceLastPromotion']\n",
    "    \n",
    "    if 'YearsAtCompany' in df.columns and 'YearsSinceLastPromotion' in df.columns:\n",
    "        df['PromotionRate'] = df['YearsAtCompany'] / (df['YearsSinceLastPromotion'] + 1)\n",
    "    \n",
    "    if 'YearsInCurrentRole' in df.columns:\n",
    "        df['CareerStagnation'] = (df['YearsInCurrentRole'] > 5).astype(int)\n",
    "    \n",
    "    if 'MonthlyIncome' in df.columns and 'YearsAtCompany' in df.columns:\n",
    "        df['IncomePerYear'] = df['MonthlyIncome'] * 12 / (df['YearsAtCompany'] + 1)\n",
    "    \n",
    "    if 'PercentSalaryHike' in df.columns:\n",
    "        df['IncomeGrowthRate'] = df['PercentSalaryHike'] / 100\n",
    "    \n",
    "    if 'duration_hours' in df.columns:\n",
    "        df['AvgDailyHours'] = df['duration_hours'] / 260\n",
    "        df['Overwork'] = (df['AvgDailyHours'] > 9).astype(int)\n",
    "        df['Overtime'] = (df['AvgDailyHours'] > 8).astype(int)\n",
    "    \n",
    "    weekend_cols = ['worked_on_day_5', 'worked_on_day_6'] # Saturday and Sunday\n",
    "    if all(col in df.columns for col in weekend_cols):\n",
    "        weekend_work = df['worked_on_day_5'] + df['worked_on_day_6']\n",
    "        df['WeekendWorker'] = (weekend_work > 10).astype(int) # worked more than 10 weekends\n",
    "    \n",
    "    satisfaction_cols = ['JobSatisfaction', 'EnvironmentSatisfaction', 'WorkLifeBalance']\n",
    "    if all(col in df.columns for col in satisfaction_cols):\n",
    "        df['OverallSatisfaction'] = (\n",
    "            df['JobSatisfaction'] + \n",
    "            df['EnvironmentSatisfaction'] + \n",
    "            df['WorkLifeBalance']\n",
    "        ) / 3\n",
    "        df['LowSatisfaction'] = (df['OverallSatisfaction'] < 2).astype(int)\n",
    "\n",
    "    # advanced indicator\n",
    "    if all(col in df.columns for col in ['YearsWithoutPromotion', 'JobSatisfaction', 'WorkLifeBalance']):\n",
    "        df['HighRiskProfile'] = (\n",
    "            (df['YearsWithoutPromotion'] > 3) & \n",
    "            (df['JobSatisfaction'] < 3) &\n",
    "            (df['WorkLifeBalance'] < 3)\n",
    "        ).astype(int)\n",
    "    \n",
    "    if 'TotalWorkingYears' in df.columns and 'MonthlyIncome' in df.columns:\n",
    "        df['ExperienceIncomeRatio'] = df['TotalWorkingYears'] / (df['MonthlyIncome'] / 1000 + 1)\n",
    "    \n",
    "    if 'NumCompaniesWorked' in df.columns and 'TotalWorkingYears' in df.columns:\n",
    "        df['JobChangeFrequency'] = df['NumCompaniesWorked'] / (df['TotalWorkingYears'] + 1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply feature engineering BEFORE split to avoid leakage\n",
    "raw_dataset = create_advanced_features(raw_dataset)\n",
    "print(f\"Features after engineering: {len(raw_dataset.columns)} columns\")\n",
    "raw_dataset.drop(columns=[\"EmployeeID\"], inplace=True)\n",
    "print(\"Advanced features created successfully\")\n",
    "\n",
    "# Split BEFORE any preprocessing to avoid data leakage\n",
    "train_set, test_set = train_test_split(raw_dataset, test_size=0.2, random_state=random_state, stratify=raw_dataset['Attrition'])\n",
    "\n",
    "print(f\"Train set: {len(train_set)} samples\")\n",
    "print(f\"Test set: {len(test_set)} samples\")\n",
    "print(f\"Train class distribution: {train_set['Attrition'].value_counts().to_dict()}\")\n",
    "print(f\"Test class distribution: {test_set['Attrition'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e47172",
   "metadata": {},
   "source": [
    "\n",
    "This block enhances the dataset's predictive power by deriving new business-relevant metrics and then partitioning the data for validation.\n",
    "\n",
    "* **Feature Engineering:** The `create_advanced_features` function generates complex indicators based on HR logic, including:\n",
    "    * **Career Progression:** Metrics like `PromotionRate` and `CareerStagnation` to detect lack of growth.\n",
    "    * **Work-Life Balance:** Variables derived from time logs, such as `AvgDailyHours`, `Overtime` (working >8h), and `WeekendWorker`.\n",
    "    * **Composite Scores:** `OverallSatisfaction` combines multiple survey results, and `HighRiskProfile` flags employees showing multiple warning signs simultaneously.\n",
    "* **Data Splitting:**\n",
    "    * Removes the non-predictive `EmployeeID` column.\n",
    "    * Performs a **Stratified Train/Test Split** (80% training, 20% testing). Stratification is critical here to ensure the minority class (\"Yes\" for Attrition) is represented proportionally in both sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba87b597",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b8643d19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = raw_dataset.copy()\n",
    "counts = df[\"Attrition\"].value_counts().reindex([\"No\", \"Yes\"])\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.bar(counts.index.astype(str), counts.values)\n",
    "plt.title(\"Attrition (count)\")\n",
    "plt.xlabel(\"Attrition\")\n",
    "plt.ylabel(\"Employees\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76faa82",
   "metadata": {},
   "source": [
    "Based on the **attrition graph**, we observe that the target variable (**Attrition**) is **imbalanced**: there are **many more “No”** cases than **“Yes”** cases (employees who left are the minority class).\n",
    "\n",
    "**Implication:** if we train a model on this as-is, it may learn to predict mostly **“No”** and **overfit** still obtain an apparently good accuracy, while performing poorly at detecting the **“Yes”** class.\n",
    "\n",
    "**Planned fix (later, during modeling):** we will apply a rebalancing technique such as **SMOTE** to generate additional synthetic samples of the minority class. This will be done **only on the training set** (after the train/test split) to avoid data leakage and keep the test set representative of real-world proportions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0bcc26",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6e911e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data_safe(dataset, fitted_scaler=None, fitted_imputers=None, fit_mode=True, \n",
    "                          encode_ordinal_cols=None, remove_from_encoding=[]):\n",
    "    \"\"\"\n",
    "    Preprocess data with proper train/test separation.\n",
    "    \n",
    "    Args:\n",
    "        dataset: DataFrame to preprocess\n",
    "        fitted_scaler: Pre-fitted StandardScaler (for test set)\n",
    "        fitted_imputers: Dict of pre-fitted imputation values (for test set)\n",
    "        fit_mode: If True, fit transformers. If False, use provided transformers\n",
    "        encode_ordinal_cols: Dict of ordinal encodings\n",
    "        remove_from_encoding: Columns to exclude from encoding\n",
    "    \n",
    "    Returns:\n",
    "        data: Preprocessed DataFrame\n",
    "        scaler: Fitted StandardScaler\n",
    "        imputers: Dict of imputation values\n",
    "    \"\"\"\n",
    "    data = dataset.copy()\n",
    "    \n",
    "    # remove useless columns\n",
    "    constant_cols = [col for col in data.columns if data[col].nunique() <= 1]\n",
    "    if constant_cols:\n",
    "        data.drop(columns=constant_cols, inplace=True)\n",
    "    \n",
    "    numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    categorical_cols = data.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "\n",
    "    if 'EmployeeID' in numeric_cols:\n",
    "        numeric_cols.remove('EmployeeID')\n",
    "\n",
    "    # remove target col from X\n",
    "    if 'Attrition' in numeric_cols:\n",
    "        numeric_cols.remove('Attrition')\n",
    "    \n",
    "    # imputation\n",
    "    if fit_mode:\n",
    "        # fit\n",
    "        imputers = {}\n",
    "        if len(numeric_cols) > 0:\n",
    "            imputers['numeric'] = data[numeric_cols].median()\n",
    "        if len(categorical_cols) > 0:\n",
    "            imputers['categorical'] = data[categorical_cols].mode().iloc[0] if len(data[categorical_cols].mode()) > 0 else {}\n",
    "    else:\n",
    "        # transform\n",
    "        imputers = fitted_imputers\n",
    "    \n",
    "    if len(numeric_cols) > 0 and 'numeric' in imputers:\n",
    "        data[numeric_cols] = data[numeric_cols].fillna(imputers['numeric'])\n",
    "    if len(categorical_cols) > 0 and 'categorical' in imputers:\n",
    "        data[categorical_cols] = data[categorical_cols].fillna(imputers['categorical'])\n",
    "    \n",
    "    # ordinal\n",
    "    if encode_ordinal_cols:\n",
    "        for col, categories in encode_ordinal_cols.items():\n",
    "            if col in data.columns:\n",
    "                data[col] = pd.Categorical(data[col], categories=categories, ordered=True).codes\n",
    "                if col in categorical_cols:\n",
    "                    categorical_cols.remove(col)\n",
    "                if col not in numeric_cols:\n",
    "                    numeric_cols.append(col)\n",
    "    \n",
    "    # one-hot\n",
    "    cols_to_encode = [col for col in categorical_cols if col not in remove_from_encoding]\n",
    "    if len(cols_to_encode) > 0:\n",
    "        data = pd.get_dummies(data, columns=cols_to_encode, drop_first=True)\n",
    "\n",
    "    # after encoding categorical, recover numeric\n",
    "    numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if 'Attrition' in numeric_cols:\n",
    "        numeric_cols.remove('Attrition')\n",
    "    if 'EmployeeID' in numeric_cols:\n",
    "        numeric_cols.remove('EmployeeID')\n",
    "    \n",
    "    # scale numeric\n",
    "    if fit_mode:\n",
    "        # fit\n",
    "        scaler = StandardScaler()\n",
    "        if len(numeric_cols) > 0:\n",
    "            data[numeric_cols] = scaler.fit_transform(data[numeric_cols])\n",
    "    else:\n",
    "        # transform\n",
    "        scaler = fitted_scaler\n",
    "        if len(numeric_cols) > 0 and scaler is not None:\n",
    "            data[numeric_cols] = scaler.transform(data[numeric_cols])\n",
    "    \n",
    "    return data, scaler, imputers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43afde6",
   "metadata": {},
   "source": [
    "\n",
    "This function (`preprocess_data_safe`) prepares the raw data for machine learning while strictly preventing **data leakage** (ensuring information from the test set does not influence the training process). It operates in two modes: `fit_mode=True` (learning from training data) or `fit_mode=False` (applying learned parameters to test data).\n",
    "\n",
    "**Key steps include:**\n",
    "* **Cleaning:** Removes columns with constant values (zero variance).\n",
    "* **Imputation:** Fills missing values using the *median* for numerical columns and the *mode* for categorical columns (calculated solely on training data).\n",
    "* **Encoding:** Converts categorical variables into numbers using **Ordinal Encoding** (for ranked data) and **One-Hot Encoding** (for nominal data like Job Role).\n",
    "* **Scaling:** Standardizes numerical features (Mean=0, Std=1) using `StandardScaler` to ensure all features contribute equally to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce30eebc",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5f4a2612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Preprocessing Train and Test Sets Separately\n",
      "======================================================================\n",
      "Train set preprocessed: (3528, 294)\n",
      "Test set preprocessed: (882, 294)\n",
      "NO DATA LEAKAGE: Scaler and imputers fitted only on train set\n",
      "\n",
      "Final feature count: 290\n",
      "X_train shape: (3528, 290)\n",
      "X_test shape: (882, 290)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Preprocessing Train and Test Sets Separately\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "ordinal_mappings = {\n",
    "    \"BusinessTravel\": [\"Non-Travel\", \"Travel_Rarely\", \"Travel_Frequently\"]\n",
    "}\n",
    "\n",
    "mutual_info_columns = [\"Department\", \"EducationField\", \"JobRole\"]\n",
    "\n",
    "# Attrition to numeric\n",
    "if train_set[target_col].dtype == \"object\":\n",
    "    train_set[target_col] = train_set[target_col].apply(lambda x: 1 if str(x).lower() in [\"yes\", \"1\"] else 0)\n",
    "if test_set[target_col].dtype == \"object\":\n",
    "    test_set[target_col] = test_set[target_col].apply(lambda x: 1 if str(x).lower() in [\"yes\", \"1\"] else 0)\n",
    "\n",
    "# preprocess training\n",
    "train_processed, fitted_scaler, fitted_imputers = preprocess_data_safe(\n",
    "    train_set,\n",
    "    fitted_scaler=None,\n",
    "    fitted_imputers=None,\n",
    "    fit_mode=True,\n",
    "    encode_ordinal_cols=ordinal_mappings,\n",
    "    remove_from_encoding=[\"Attrition\"] + mutual_info_columns\n",
    ")\n",
    "print(f\"Train set preprocessed: {train_processed.shape}\")\n",
    "\n",
    "# preprocess testing from fitted training transformers\n",
    "test_processed, _, _ = preprocess_data_safe(\n",
    "    test_set,\n",
    "    fitted_scaler=fitted_scaler,\n",
    "    fitted_imputers=fitted_imputers,\n",
    "    fit_mode=False,\n",
    "    encode_ordinal_cols=ordinal_mappings,\n",
    "    remove_from_encoding=[\"Attrition\"] + mutual_info_columns\n",
    ")\n",
    "print(f\"Test set preprocessed: {test_processed.shape}\")\n",
    "print(f\"NO DATA LEAKAGE: Scaler and imputers fitted only on train set\")\n",
    "\n",
    "# prepare X and y\n",
    "X_train_full = train_processed.drop(columns=[target_col] + [col for col in mutual_info_columns if col in train_processed.columns], errors=\"ignore\")\n",
    "X_test_full = test_processed.drop(columns=[target_col] + [col for col in mutual_info_columns if col in test_processed.columns], errors=\"ignore\")\n",
    "y_train = train_processed[target_col]\n",
    "y_test = test_processed[target_col]\n",
    "\n",
    "common_cols = X_train_full.columns.intersection(X_test_full.columns)\n",
    "X_train = X_train_full[common_cols]\n",
    "X_test = X_test_full[common_cols]\n",
    "\n",
    "print(f\"\\nFinal feature count: {len(common_cols)}\")\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ecc50c",
   "metadata": {},
   "source": [
    "\n",
    "This block executes the data transformation pipeline while strictly adhering to machine learning best practices:\n",
    "\n",
    "* **Target Encoding:** Converts the categorical target variable `Attrition` (\"Yes\"/\"No\") into a binary format (1/0) for algorithmic processing.\n",
    "* **Sequential Preprocessing:**\n",
    "    * **Train Set:** Runs in `fit_mode=True` to learn scaling parameters (mean, std) and imputation values from training data only.\n",
    "    * **Test Set:** Runs in `fit_mode=False` to apply those learned parameters to the test data. This guarantees that the model never \"sees\" the test distribution during training (preventing data leakage).\n",
    "* **Feature Alignment:** Finalizes the $X$ (features) and $y$ (target) matrices, ensuring that both training and testing datasets share the exact same column structure (handling any discrepancies in one-hot encoding)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d0b131",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "52ef0eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Feature Selection Using ANOVA (on TRAIN set only)\n",
      "======================================================================\n",
      "\n",
      "Top Features by ANOVA F-score:\n",
      "duration_hours             148.264763\n",
      "AvgDailyHours              148.264763\n",
      "JobChangeFrequency         141.853787\n",
      "TotalWorkingYears          113.629119\n",
      "Overtime                   112.514236\n",
      "YearsWithCurrManager        96.353514\n",
      "OverallSatisfaction         88.951704\n",
      "YearsAtCompany              82.577830\n",
      "PromotionRate               80.537343\n",
      "Overwork                    80.431557\n",
      "IncomePerYear               67.710589\n",
      "BusinessTravel              53.970471\n",
      "JobSatisfaction             40.254561\n",
      "EnvironmentSatisfaction     34.818897\n",
      "ExperienceIncomeRatio       33.587992\n",
      "LowSatisfaction             32.961292\n",
      "WorkLifeBalance             13.434350\n",
      "TrainingTimesLastYear        7.760851\n",
      "YearsWithoutPromotion        6.379768\n",
      "YearsSinceLastPromotion      6.379768\n",
      "dtype: float64\n",
      "\n",
      "Selected 36 features\n",
      "X_train: (3528, 290) → (3528, 36)\n",
      "X_test: (882, 290) → (882, 36)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Feature Selection Using ANOVA (on TRAIN set only)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# calculate ANOVA on training set only (no leakage)\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "# get scores\n",
    "selector = SelectKBest(f_classif, k='all')\n",
    "selector.fit(X_train, y_train)\n",
    "feature_scores = pd.Series(selector.scores_, index=X_train.columns)\n",
    "feature_scores = feature_scores.sort_values(ascending=False)\n",
    "\n",
    "# filter out noise\n",
    "exclude_patterns = [\"day_of_week\", \"avg_hours_day_\", r\"\\d{4}-\\d{2}-\\d{2}_hours\"]\n",
    "for pattern in exclude_patterns:\n",
    "    feature_scores = feature_scores[~feature_scores.index.str.contains(pattern, regex=True)]\n",
    "\n",
    "print(f\"\\nTop Features by ANOVA F-score:\")\n",
    "print(feature_scores.head(20))\n",
    "\n",
    "# select top features\n",
    "top_k = len(feature_scores) \n",
    "top_features = feature_scores.head(top_k).index.tolist()\n",
    "print(f\"\\nSelected {top_k} features\")\n",
    "\n",
    "# apply feature selection\n",
    "X_train_selected = X_train[top_features]\n",
    "X_test_selected = X_test[top_features]\n",
    "\n",
    "# reorder columns based on feature importance\n",
    "X_train_selected = X_train_selected[feature_scores.head(top_k).index]\n",
    "X_test_selected = X_test_selected[feature_scores.head(top_k).index]\n",
    "\n",
    "print(f\"X_train: {X_train.shape} → {X_train_selected.shape}\")\n",
    "print(f\"X_test: {X_test.shape} → {X_test_selected.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ece483",
   "metadata": {},
   "source": [
    "\n",
    "This part identifies the most relevant variables to predict employee turnover using statistical testing:\n",
    "\n",
    "* **ANOVA Analysis:** Uses the **Analysis of Variance (F-test)** to score each feature based on how strongly it discriminates between employees who stay and those who leave. Crucially, this is calculated *only* on the training set to maintain validity.\n",
    "* **Noise Filtering:** Explicitly removes granular, noisy columns (like raw daily timestamps or specific days of the week) to focus the model on the meaningful aggregated metrics created earlier (e.g., `AvgDailyHours`).\n",
    "* **Ranking:** Sorts the remaining features by importance and subsets the data, ensuring the model prioritizes the strongest predictors (like tenure, overtime, or age) while ignoring irrelevant noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1f61d8",
   "metadata": {},
   "source": [
    "## Sythentic Minority Oversampling Technique (SMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "fbc7e845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Nested Cross-Validation for SMOTE Strategy\n",
      "======================================================================\n",
      "\n",
      "Testing SMOTE strategies with nested cross-validation...\n",
      "\n",
      "Best SMOTE strategy: 0.54 (CV F1=0.822214)\n",
      "\n",
      "Class distribution:\n",
      "  Original: {0: 2959, 1: 569}\n",
      "  After SMOTE: {0: 2959, 1: 1597}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Nested Cross-Validation for SMOTE Strategy\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# nested CV for more robust SMOTE strategy selection\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "# hyperparameter tuning\n",
    "inner_cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=random_state)\n",
    "\n",
    "smote_strategies = [\n",
    "    0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49,\n",
    "    0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59,\n",
    "    0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69,\n",
    "    0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79,\n",
    "    0.80,\n",
    "    ]\n",
    "smote_results = {}\n",
    "\n",
    "print(\"\\nTesting SMOTE strategies with nested cross-validation...\")\n",
    "for strategy in smote_strategies:\n",
    "    # create pipeline to prevent leakage\n",
    "    pipeline = ImbPipeline([\n",
    "        ('smote', SMOTE(random_state=random_state, k_neighbors=10, sampling_strategy=strategy)),\n",
    "        ('classifier', RandomForestClassifier(n_estimators=50, random_state=random_state, n_jobs=-1))\n",
    "    ])\n",
    "    \n",
    "    # cross-validate on training set\n",
    "    cv_scores = cross_val_score(pipeline, X_train_selected, y_train, cv=inner_cv, scoring='f1', n_jobs=-1)\n",
    "    smote_results[strategy] = cv_scores.mean()\n",
    "    \n",
    "# select best strategy based on CV performance\n",
    "best_smote_strategy = max(smote_results, key=smote_results.get)\n",
    "best_smote_f1 = smote_results[best_smote_strategy]\n",
    "\n",
    "print(f\"\\nBest SMOTE strategy: {best_smote_strategy} (CV F1={best_smote_f1:.6f})\")\n",
    "\n",
    "# apply SMOTE with best strategy\n",
    "smote = SMOTE(random_state=random_state, k_neighbors=10, sampling_strategy=best_smote_strategy)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train_selected, y_train)\n",
    "\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(f\"  Original: {pd.Series(y_train).value_counts().sort_index().to_dict()}\")\n",
    "print(f\"  After SMOTE: {pd.Series(y_train_balanced).value_counts().sort_index().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b16c87",
   "metadata": {},
   "source": [
    "\n",
    "In here the code addresses the significant class imbalance (far fewer employees leave, many more stay) using a data-driven approach rather than a fixed rule:\n",
    "\n",
    "* **Pipeline Validation:** It iterates through various sampling strategies (ratios from 0.40 to 0.80) inside a **Pipeline**. This ensures that synthetic data generation (SMOTE) occurs *only* on the training folds during cross-validation, preventing data leakage into the validation folds.\n",
    "* **Optimization:** It evaluates each strategy using the **F1-score** (which balances precision and recall) to find the optimal ratio of minority-to-majority class samples.\n",
    "* **Final Resampling:** Once the best ratio is identified, it applies SMOTE to the full training set, creating a balanced dataset (`X_train_balanced`) ready for the final model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e0ea53",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5917ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Random Forest Model Training with Optimized Hyperparameters\n",
      "======================================================================\n",
      "\n",
      "Optimizing Random Forest...\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n"
     ]
    }
   ],
   "source": [
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from copy import deepcopy\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Random Forest Model Training with Optimized Hyperparameters\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# optimize Random Forest\n",
    "print(\"\\nOptimizing Random Forest...\")\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [5, 10, 15, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "}\n",
    "\n",
    "grid_search_rf = GridSearchCV(\n",
    "    RandomForestClassifier(random_state=random_state, n_jobs=-1),\n",
    "    param_grid_rf,\n",
    "    cv=5,\n",
    "    scoring='f1',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "grid_search_rf.fit(X_train_balanced, y_train_balanced)\n",
    "print(f\"\\nBest Random Forest CV F1: {grid_search_rf.best_score_:.3f}\")\n",
    "print(\"\\nBest parameters:\")\n",
    "for param, value in grid_search_rf.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "rf_opt = grid_search_rf.best_estimator_\n",
    "\n",
    "# CV the best model\n",
    "rf_cv_scores = cross_val_score(rf_opt, X_train_balanced, y_train_balanced, cv=5, scoring='f1')\n",
    "print(f\"\\nRandom Forest CV F1: {rf_cv_scores.mean():.3f} (std={rf_cv_scores.std():.3f})\")\n",
    "\n",
    "models = {\n",
    "    \"RandomForest_Optimized\": rf_opt,\n",
    "}\n",
    "\n",
    "# store grid_search for compatibility\n",
    "grid_search = grid_search_rf\n",
    "\n",
    "for name, m in models.items():\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"{name} - EVALUATION RESULTS\")\n",
    "    print(\"=\"*70)\n",
    "    t_start = time.time()\n",
    "    \n",
    "    # use F1 score for CV instead of accuracy (better for imbalanced data)\n",
    "    x_val_scores_f1 = cross_val_score(m, X_train_balanced, y_train_balanced, cv=5, scoring=\"f1\")\n",
    "    print(f\"\\nCross-validation F1 mean {x_val_scores_f1.mean():.4f}, std dev {x_val_scores_f1.std():.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    print(\"Probability Calibration\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    # split training data\n",
    "    X_train_cal, X_val_cal, y_train_cal, y_val_cal = train_test_split(\n",
    "        X_train_balanced, y_train_balanced,\n",
    "        test_size=0.2,\n",
    "        random_state=random_state,\n",
    "        stratify=y_train_balanced\n",
    "    )\n",
    "    \n",
    "    # Calibrate probabilities\n",
    "    m_uncal = deepcopy(m)\n",
    "    m_uncal.fit(X_train_cal, y_train_cal)\n",
    "    calibrated_model = CalibratedClassifierCV(\n",
    "        m_uncal,\n",
    "        method='sigmoid',\n",
    "        cv='prefit'\n",
    "    )\n",
    "    calibrated_model.fit(X_val_cal, y_val_cal)\n",
    "    print(f\"Model calibrated\")\n",
    "    \n",
    "    y_pred = calibrated_model.predict(X_test_selected)\n",
    "    \n",
    "    print(\"Confusion Matrix:\")\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(cm)\n",
    "    \n",
    "    # calculate key metrics\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    precision_class1 = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall_class1 = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1_class1 = 2 * (precision_class1 * recall_class1) / (precision_class1 + recall_class1) if (precision_class1 + recall_class1) > 0 else 0\n",
    "    \n",
    "    print(f\"\\nAttrition Metrics:\")\n",
    "    print(f\"  Precision: {precision_class1:.3f} ({tp}/{tp+fp}) - How many alerts are real\")\n",
    "    print(f\"  Recall:    {recall_class1:.3f} ({tp}/{tp+fn}) - How many attritions detected\")\n",
    "    print(f\"  F1-Score:  {f1_class1:.3f} - Balance between precision/recall\")\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # probability scores\n",
    "    if hasattr(calibrated_model, 'predict_proba'):\n",
    "        y_prob = calibrated_model.predict_proba(X_test_selected)[:, 1]\n",
    "        print(f\"\\nUsing calibrated probabilities for threshold optimization\")\n",
    "    elif hasattr(calibrated_model, 'decision_function'):\n",
    "        y_prob = calibrated_model.decision_function(X_test_selected)\n",
    "        print(f\"\\nUsing calibrated decision scores for threshold optimization\")\n",
    "    else:\n",
    "        print(\"Warning: Model doesn't support probability prediction, skipping ROC-AUC\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\nROC-AUC Score: {roc_auc_score(y_test, y_prob):.4f}\")\n",
    "    fpr, tpr, thresholds_roc = roc_curve(y_test, y_prob)\n",
    "    roc_auc_val = auc(fpr, tpr)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Optimizing Decision Threshold\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    precisions, recalls, thresholds = precision_recall_curve(y_test, y_prob)\n",
    "    f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-10)\n",
    "    \n",
    "    # maximize F1\n",
    "    optimal_idx_f1 = np.argmax(f1_scores)\n",
    "    optimal_threshold_f1 = thresholds[optimal_idx_f1] if optimal_idx_f1 < len(thresholds) else 0.0\n",
    "    \n",
    "    # find feasible Precision\n",
    "    min_precision_threshold = 0.35\n",
    "    valid_indices = precisions >= min_precision_threshold\n",
    "    \n",
    "    if valid_indices.any():\n",
    "        valid_f1s = f1_scores[valid_indices]\n",
    "        if len(valid_f1s) > 0:\n",
    "            best_valid_idx = np.argmax(valid_f1s)\n",
    "            original_indices = np.where(valid_indices)[0]\n",
    "            optimal_idx_constrained = original_indices[best_valid_idx]\n",
    "            optimal_threshold_constrained = thresholds[optimal_idx_constrained] if optimal_idx_constrained < len(thresholds) else 0.0\n",
    "        else:\n",
    "            optimal_threshold_constrained = optimal_threshold_f1\n",
    "    else:\n",
    "        optimal_threshold_constrained = optimal_threshold_f1\n",
    "    \n",
    "    optimal_threshold = optimal_threshold_constrained\n",
    "    \n",
    "    y_pred_optimized = (y_prob >= optimal_threshold).astype(int)\n",
    "    cm_opt = confusion_matrix(y_test, y_pred_optimized)\n",
    "    tn_opt, fp_opt, fn_opt, tp_opt = cm_opt.ravel()\n",
    "    precision_opt = tp_opt / (tp_opt + fp_opt) if (tp_opt + fp_opt) > 0 else 0\n",
    "    recall_opt = tp_opt / (tp_opt + fn_opt) if (tp_opt + fn_opt) > 0 else 0\n",
    "    f1_opt = 2 * (precision_opt * recall_opt) / (precision_opt + recall_opt) if (precision_opt + recall_opt) > 0 else 0\n",
    "    \n",
    "    improvement_pct = ((f1_opt - f1_class1) / f1_class1 * 100) if f1_class1 > 0 else 0\n",
    "    \n",
    "    print(f\"\\nDEFAULT Threshold (0.5):\")\n",
    "    print(f\"   Precision: {precision_class1:.3f} | Recall: {recall_class1:.3f} | F1: {f1_class1:.3f}\")\n",
    "    print(f\"   Confusion Matrix: [[{tn:3d} {fp:3d}] [{fn:3d} {tp:3d}]]\")\n",
    "    \n",
    "    print(f\"\\nOPTIMAL Threshold ({optimal_threshold:.4f}) - With Precision >= 35% constraint:\")\n",
    "    print(f\"   Precision: {precision_opt:.3f} ({tp_opt}/{tp_opt+fp_opt}) - {precision_opt*100:.1f}% of alerts are real\")\n",
    "    print(f\"   Recall:    {recall_opt:.3f} ({tp_opt}/{tp_opt+fn_opt}) - Detects {recall_opt*100:.1f}% of attritions\")\n",
    "    print(f\"   F1-Score:  {f1_opt:.3f} -> +{improvement_pct:.1f}% improvement\")\n",
    "    print(f\"   Confusion Matrix: [[{tn_opt:3d} {fp_opt:3d}] [{fn_opt:3d} {tp_opt:3d}]]\")\n",
    "    \n",
    "    print(f\"\\nBusiness Impact:\")\n",
    "    print(f\"   Detected: {tp_opt}/{tp_opt+fn_opt} attritions ({recall_opt*100:.1f}%)\")\n",
    "    print(f\"   Cost: Need to interview {tp_opt+fp_opt} employees ({fp_opt} unnecessary)\")\n",
    "    print(f\"   Efficiency: {precision_opt*100:.1f}% of interventions are useful\")\n",
    "    print(f\"   Missed: {fn_opt} attritions will leave undetected\")\n",
    "    print(f\"   Detected: {tp_opt}/{tp_opt+fn_opt} attritions ({recall_opt*100:.1f}%)\")\n",
    "    print(f\"   Cost: Need to interview {tp_opt+fp_opt} employees ({fp_opt} unnecessary)\")\n",
    "    print(f\"   Efficiency: {precision_opt*100:.1f}% of interventions are useful\")\n",
    "    print(f\"   Missed: {fn_opt} attritions will leave undetected\")\n",
    "\n",
    "    print(f\"\\nTime taken: {time.time() - t_start:.2f} seconds\")\n",
    "\n",
    "    distances = np.sqrt(fpr**2 + (1 - tpr)**2)\n",
    "    min_idx = np.argmin(distances)\n",
    "    min_distance = distances[min_idx]\n",
    "    closest_fpr = fpr[min_idx]\n",
    "    closest_tpr = tpr[min_idx]\n",
    "    \n",
    "    # find optimal threshold point on ROC curve\n",
    "    optimal_fpr_idx = np.argmin(np.abs(thresholds_roc - optimal_threshold)) if len(thresholds_roc) > 0 else 0\n",
    "    optimal_fpr = fpr[optimal_fpr_idx] if optimal_fpr_idx < len(fpr) else 0\n",
    "    optimal_tpr = tpr[optimal_fpr_idx] if optimal_fpr_idx < len(tpr) else 1\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.plot(fpr, tpr, 'b-', linewidth=2, label=f\"ROC Curve (AUC = {roc_auc_val:.4f})\")\n",
    "    plt.plot([0, 1], [0, 1], \"k--\", label=\"Random\")\n",
    "\n",
    "    # closest point to (0,1)\n",
    "    plt.plot([0, closest_fpr], [1, closest_tpr], \"r-\", linewidth=2, alpha=0.5,\n",
    "            label=f\"Min. d Point ({closest_fpr:.3f}, {closest_tpr:.3f})\")\n",
    "    \n",
    "    # optimal threshold point\n",
    "    plt.plot(optimal_fpr, optimal_tpr, \"go\", markersize=12, \n",
    "            label=f\"Optimized Threshold={optimal_threshold:.3f}\\nFPR={optimal_fpr:.3f}, TPR={optimal_tpr:.3f}\")\n",
    "\n",
    "    # distance annotation\n",
    "    mid_x = closest_fpr / 2\n",
    "    mid_y = (1 + closest_tpr) / 2\n",
    "    plt.text(mid_x, mid_y, f\"d = {min_distance:.4f}\",\n",
    "            fontsize=9, color=\"red\",\n",
    "            bbox=dict(boxstyle=\"round\", facecolor=\"white\", edgecolor=\"red\", alpha=0.7)\n",
    "            )\n",
    "\n",
    "    plt.xlim([-0.002, 1.0])\n",
    "    plt.ylim([0.0, 1.005])\n",
    "    plt.xlabel(\"False Positive Rate\", fontsize=12)\n",
    "    plt.ylabel(\"True Positive Rate\", fontsize=12)\n",
    "    plt.title(f\"{name} - ROC Curve\\nF1={f1_opt:.3f}, Precision={precision_opt:.3f}, Recall={recall_opt:.3f}\", fontsize=14)\n",
    "    plt.legend(loc=\"lower right\", fontsize=10)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Optimization Complete\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "importances = rf_opt.feature_importances_\n",
    "feature_names = X_train_selected.columns\n",
    "feature_importances = pd.Series(importances, index=feature_names).sort_values(ascending=False)\n",
    "print(\"\\nTop Features from Optimized Random Forest:\")\n",
    "print(feature_importances)\n",
    "\n",
    "plt.bar(feature_importances.head(len(feature_importances)).index, feature_importances.head(len(feature_importances)).values)\n",
    "plt.axhline(y=np.mean(importances), color='red', linestyle='--', label='Mean Importance')\n",
    "plt.title(\"Feature Importance\")\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Importance Score\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "above_mean = feature_importances[feature_importances > np.mean(importances)]\n",
    "plt.bar(above_mean.index, above_mean.values)\n",
    "plt.title(\"Features with Above Mean Importance\")\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Importance Score\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Comparison of ANOVA F-scores vs Random Forest Importances\n",
    "# Normalize both metrics to [0, 1] for better comparison\n",
    "feature_scores_normalized = (feature_scores - feature_scores.min()) / (feature_scores.max() - feature_scores.min())\n",
    "feature_importances_normalized = (feature_importances - feature_importances.min()) / (feature_importances.max() - feature_importances.min())\n",
    "\n",
    "# Get common features and calculate total score\n",
    "common_features = feature_importances.head(15).index\n",
    "anova_scores = feature_scores_normalized[common_features]\n",
    "rf_scores = feature_importances_normalized[common_features]\n",
    "\n",
    "# Calculate total score and sort by it\n",
    "total_scores = anova_scores + rf_scores\n",
    "sorted_indices = total_scores.sort_values(ascending=False).index\n",
    "anova_scores_sorted = anova_scores[sorted_indices]\n",
    "rf_scores_sorted = rf_scores[sorted_indices]\n",
    "\n",
    "# Create stacked bar plot\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "x = np.arange(len(sorted_indices))\n",
    "\n",
    "# Stack the bars\n",
    "bars1 = ax.bar(x, anova_scores_sorted.values, label='ANOVA F-score', color='steelblue', alpha=0.8)\n",
    "bars2 = ax.bar(x, rf_scores_sorted.values, bottom=anova_scores_sorted.values, label='RF Importance', color='coral', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Features', fontsize=12)\n",
    "ax.set_ylabel('Cumulative Normalized Score', fontsize=12)\n",
    "ax.set_title('Comparison: ANOVA Feature Selection vs Random Forest Importance (Ordered by Total Score)', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(sorted_indices, rotation=45, ha='right')\n",
    "ax.legend(loc='upper right', fontsize=10)\n",
    "ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"\\nData preprocessing and model training script completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1070a782",
   "metadata": {},
   "source": [
    "\n",
    "This comprehensive block handles the entire modeling lifecycle, shifting from raw prediction to actionable business insights:\n",
    "\n",
    "* **Hyperparameter Tuning (Grid Search):** Instead of guessing settings, it exhaustively tests different combinations of Random Forest parameters (like tree depth and number of trees) to find the configuration that yields the highest F1-score on the balanced data.\n",
    "* **Probability Calibration:** Standard models can be \"overconfident.\" This step recalibrates the model's probability outputs so that a predicted \"70% risk of leaving\" actually corresponds to a 70% real-world probability of attrition.\n",
    "* **Strategic Threshold Optimization:** This is critical. The default threshold of 0.5 is rarely optimal for HR.\n",
    "    * The code dynamically finds a **custom threshold** that maximizes the F1-score.\n",
    "    * **Constraint:** It enforces a *minimum precision of 35%*. This ensures that while catching potential leavers (Recall), the HR team doesn't waste too much time interviewing employees who are actually happy (False Positives).\n",
    "* **Business Impact Analysis:** Translates abstract metrics into HR terms:\n",
    "    * **Efficiency:** Percentage of interventions that are useful.\n",
    "    * **Cost:** Number of unnecessary interviews conducted.\n",
    "    * **Risk:** Number of resignations the model failed to predict.\n",
    "* **Visualization:** Plots the **ROC Curve** to visually assess the trade-off between catching leavers and raising false alarms, marking the optimal operating point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76496811-72e3-4756-9481-16adf0403bc7",
   "metadata": {},
   "source": [
    "# Ethical Considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db2d682-4c3e-4194-9c64-1d5ad61da877",
   "metadata": {},
   "source": [
    "## Fairness: Removing Sensitive Demographic Attributes\n",
    "To reduce the risk of bias and discriminatory outcomes, we removed sensitive demographic attributes **before training**:\n",
    "- `Age`\n",
    "- `Gender`\n",
    "- `MaritalStatus` \n",
    "\n",
    "### Why These Features Were Removed (Ethical Justification)\n",
    "\n",
    "- **Age**\n",
    "  - Risk: can directly enable **age discrimination** (older/younger employees treated differently by the model).\n",
    "  - Even if age correlates with attrition, using it may produce decisions that are unfair and socially harmful.\n",
    "  - We prefer work-related explanations (workload, role, satisfaction) rather than demographic profiling.\n",
    "\n",
    "- **Gender**\n",
    "  - Risk: gender is a **protected characteristic**; including it can create or amplify unequal outcomes between groups.\n",
    "  - Gender can also act as a proxy for systemic workplace inequalities (promotion gaps, pay gaps), which the model should not reinforce.\n",
    "  - Removing it helps prevent the model from learning gender-based patterns in attrition.\n",
    "\n",
    "- **MaritalStatus**\n",
    "  - Risk: sensitive personal-life attribute that can lead to **stereotypes** (e.g., assumptions about commitment, availability, family constraints).\n",
    "  - It may act as a proxy for caregiving responsibilities or other private-life factors that should not drive organizational decisions.\n",
    "  - Excluding it keeps the analysis focused on factors the organization can address fairly (work conditions, job design, management).\n",
    "\n",
    "\n",
    "  Ethical considerations (and how they influenced our results)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Data Minimisation & Purpose Limitation\n",
    "\n",
    "We followed a data minimisation mindset: we keep and use only variables that are relevant to the project goal (understanding attrition through workplace signals) and avoid using extra personal information “just because it exists.”\n",
    "\n",
    "Impact on results: the model/analysis is more aligned with an HR-acceptable scope (work conditions, job context), and the conclusions are more defensible because they focus on factors the organisation can potentially act on without profiling employees.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Accountability: Documenting Ethical Decisions\n",
    "\n",
    "We explicitly documented what we removed and why, so the process is transparent and auditable.\n",
    "\n",
    "Impact on results: the final conclusions are shaped by these constraints (we accept that some potentially predictive information is excluded), and our results reflect a fairness-first approach rather than pure optimisation. \n",
    "\n",
    "\n",
    "\n",
    "## Ethical Impact of Prediction Errors (Confusion Matrix)\n",
    "\n",
    "Beyond overall performance, we interpret the confusion matrix ethically, because each error type has different real-world consequences:\n",
    "-\tTrue Positive (TP) (predict “Attrition = Yes” and the employee leaves): excellent, the model correctly flags a real risk.\n",
    "-\tTrue Negative (TN) (predict “Attrition = No” and the employee stays): excellent, no unnecessary intervention.\n",
    "-\tFalse Positive (FP) (predict “Yes” but the employee stays): less severe, but still sensitive — it may trigger unnecessary follow-up, create stress, or lead to unfair suspicion if mishandled.\n",
    "-\tFalse Negative (FN) (predict “No” but the employee leaves): ethically the worst case, because the model fails to detect a real risk, potentially preventing timely support or retention actions.\n",
    "\n",
    "Impact on our approach: we do not treat all errors equally. Since FN are the most harmful, we prioritise minimizing FN (e.g., focusing on Recall for “Attrition = Yes”) rather than optimizing accuracy alone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cee74c8-712e-40ec-ad15-8cf8924f3fa3",
   "metadata": {},
   "source": [
    "# Bibliography"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefdefd8",
   "metadata": {},
   "source": [
    "## 1. Methodological and Theoretical Sources\n",
    "\n",
    "1. **Wirth, R., & Hipp, J. (2000). _CRISP-DM: Towards a standard process model for data mining._**  \n",
    "   **Annotation:** This source provided the structural framework for our project lifecycle. It guided us through the stages of Business Understanding (reducing the 15% turnover rate), Data Preparation (merging the four CSV files and time logs), and Model Evaluation.  \n",
    "   **Copyright:** Open industry standard.  \n",
    "   **Reference:** CRISP-DM: Towards a Standard Process Model for Data Mining\n",
    "By Rüdiger Wirth, Jochen HippYear: 2000URL: http://cs.unibo.it/~danilo.montesi/CBD/Beatriz/10.1.1.198.5133.pdf\n",
    "\n",
    "1. **European Commission (2019). _Ethics Guidelines for Trustworthy AI._**  \n",
    "   **Annotation:** This is the primary reference for our Ethics Deliverable. We used the *Assessment List for Trustworthy AI* to justify the removal of sensitive features (`Age`, `Gender`, `Marital Status`) and to establish our *Human-in-the-loop* decision-making strategy.  \n",
    "   **Copyright:** Published under Creative Commons Attribution 4.0 International (CC BY 4.0).  \n",
    "   **Reference:** Ethics Guidelines for Trustworthy AI | Shaping Europe’s Digital Future. https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai. Accessed 18 Dec. 2025.\n",
    "\n",
    "## 2. Sources on Technical Aspects\n",
    "\n",
    "1. **Pedregosa, F., et al. (2011). _Scikit-learn: Machine Learning in Python._**  \n",
    "   **Annotation:** Crucial for the implementation of the `RandomForestClassifier`, `StandardScaler`, and ANOVA (`f_classif`). Specifically, we used the documentation on Platt Scaling (`CalibratedClassifierCV`) to optimize our prediction thresholds, ensuring that HR receives reliable probability scores.  \n",
    "   **Copyright:** BSD 3-Clause License.  \n",
    "   **Reference:** 1.16. Probability Calibration’. Scikit-Learn, https://scikit-learn/stable/modules/calibration.html. Accessed 18 Dec. 2025.\n",
    "\n",
    "1. **Chawla, N. V., et al. (2002). _SMOTE: Synthetic Minority Over-sampling Technique._**  \n",
    "   **Annotation:** Since the attrition rate was imbalanced (only 15%), we utilized SMOTE to balance the training set. This paper provided the theoretical justification for creating synthetic samples to improve the model’s F1-Score and Recall.  \n",
    "   **Copyright:** Open-access academic publication.  \n",
    "   **Reference:** SMOTE: Synthetic Minority Over-sampling Technique\n",
    "By N. V. Chawla, K. W. Bowyer, https://arxiv.org/pdf/1106.1813, L. O. Hall, W. P. KegelmeyerContainer: Journal of Artificial Intelligence ResearchYear: 2002Volume: 16Issue: 16DOI: 10.1613/jair.953\n",
    "\n",
    "## 3. Ethical and Societal Sources\n",
    "\n",
    "1. **European Parliament (2016). _General Data Protection Regulation (GDPR)._**  \n",
    "   **Annotation:** This source informed our Data Governance plan. We applied principles of Data Minimization (dropping non-informative columns such as `EmployeeCount` and `Over18`) and Pseudonymization (protecting `EmployeeID`).  \n",
    "   **Copyright:** Official EU Legislative Text (Public Domain).  \n",
    "   **Reference:** General Data Protection Regulation (GDPR) – Legal Text’. General Data Protection Regulation (GDPR), https://gdpr-info.eu/. Accessed 18 Dec. 2025.\n",
    "\n",
    "1. **Barocas, S., & Selbst, A. D. (2016). _Big Data’s Disparate Impact._**  \n",
    "   **Annotation:** This article helped our team identify the risks of *proxy variables*. It justified why we must monitor features such as `MonthlyIncome` and `DistanceFromHome` to ensure they do not result in indirect discrimination.  \n",
    "   **Copyright:** Published in the *California Law Review*.  \n",
    "   **Reference:** Big Data's Disparate Impact\n",
    "By Solon Barocas, Andrew SelbstContainer: California Law ReviewYear: 2016Volume: 104Issue: 3DOI: 10.15779/Z38BG31URL: http://www.californialawreview.org/wp-content/uploads/2016/06/2Barocas-Selbst.pdf\n",
    "\n",
    "## 4. Project-Specific Sources\n",
    "\n",
    "1. **Choudhary, V. J. (2017). _HR Analytics Case Study._ Kaggle.**  \n",
    "   **Annotation:** The source of the original dataset. We used the context provided here to perform advanced feature engineering, such as deriving `AvgDailyHours` and *Overwork* indicators from the raw time logs (`in_out_time`).  \n",
    "   **Copyright:** CC0: Public Domain.  \n",
    "   **Reference:** HR Analytics Case Study\n",
    "By Vijay ChoudharyContainer: Kaggle.comYear: 2018URL: https://www.kaggle.com/datasets/vjchoudhary7/hr-analytics-case-study\n",
    "\n",
    "1. **Molnar, C. (2020). _Interpretable Machine Learning._**  \n",
    "   **Annotation:** This guide influenced our decision to use feature importance and SHAP-based explanations. It ensured that our final model provides transparency for HR managers instead of being a “black box.”  \n",
    "   **Copyright:** Published under Creative Commons Attribution–NonCommercial–ShareAlike 4.0.  \n",
    "   **Reference:** Interpretable Machine Learning. https://christophm.github.io/interpretable-ml-book/. Accessed 18 Dec. 2025."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
