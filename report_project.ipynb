{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d11f6fb6",
   "metadata": {},
   "source": [
    "## import and innit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d4fcc1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import f_classif, mutual_info_classif\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve, auc, precision_recall_curve\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "target_col = \"Attrition\"\n",
    "random_state = 42 #np.random.random()\n",
    "\n",
    "folder_path = \"data\"\n",
    "time_folder_path = \"in_out_time\"\n",
    "in_time_file_name = \"in_time.csv\"\n",
    "out_time_file_name = \"out_time.csv\"\n",
    "employee_file_name = \"employee_survey_data.csv\"\n",
    "general_file_name = \"general_data.csv\"\n",
    "manager_file_name = \"manager_survey_data.csv\"\n",
    "\n",
    "employee_data = pd.read_csv(os.path.join(folder_path, employee_file_name))\n",
    "general_data = pd.read_csv(os.path.join(folder_path, general_file_name))\n",
    "manager_data = pd.read_csv(os.path.join(folder_path, manager_file_name))\n",
    "in_time_data = pd.read_csv(os.path.join(folder_path, time_folder_path, in_time_file_name))\n",
    "out_time_data = pd.read_csv(os.path.join(folder_path, time_folder_path, out_time_file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787cebd2",
   "metadata": {},
   "source": [
    "\n",
    "This block initializes the analysis environment by importing essential libraries for:\n",
    "* **Data Manipulation:** `pandas`, `numpy`\n",
    "* **Visualization:** `matplotlib`\n",
    "* **Machine Learning:** `sklearn` (Random Forest, metrics, preprocessing), `imblearn` (SMOTE for balancing)\n",
    "\n",
    "It also configures global parameters, such as the `random_state` for reproducibility and the `target_col` (\"Attrition\"). Finally, it loads the five distinct CSV datasets provided by the HR department (General Data, Employee Survey, Manager Survey, and In/Out Time logs) into Pandas DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5fe7a3",
   "metadata": {},
   "source": [
    "## in out time handeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c23a2c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# merge in_time and out_time data on the first column (Unknown that is actually EmployeeID)\n",
    "# rename the first column to EmployeeID for both datasets because it is unnamed\n",
    "in_time_data.rename(columns={in_time_data.columns[0]: \"EmployeeID\"}, inplace=True)\n",
    "out_time_data.rename(columns={out_time_data.columns[0]: \"EmployeeID\"}, inplace=True)\n",
    "\n",
    "#check if days are present in both datasets\n",
    "in_time_days = set(in_time_data.columns[1:])\n",
    "out_time_days = set(out_time_data.columns[1:])\n",
    "missing_in_out = in_time_days.difference(out_time_days)\n",
    "# display the missing days\n",
    "print(f\"Days missing in either in_time or out_time data: {missing_in_out}\")\n",
    "\n",
    "# go through each column to check empty cells present only in one of the datasets\n",
    "for day in in_time_days.intersection(out_time_days):\n",
    "    in_time_empty = set(in_time_data.index[in_time_data[day].isnull()])\n",
    "    out_time_empty = set(out_time_data.index[out_time_data[day].isnull()])\n",
    "    missing_in_out_rows = in_time_empty.symmetric_difference(out_time_empty)\n",
    "    if missing_in_out_rows:\n",
    "        print(f\"Day {day} has missing entries in either in_time or out_time data at rows: {missing_in_out_rows}\")\n",
    "\n",
    "# convert all columns except the first one to datetime format\n",
    "for col in in_time_data.columns[1:]:\n",
    "    in_time_data[col] = pd.to_datetime(in_time_data[col], format=\"%Y-%m-%d %H:%M:%S\", errors=\"coerce\")\n",
    "for col in out_time_data.columns[1:]:\n",
    "    out_time_data[col] = pd.to_datetime(out_time_data[col], format=\"%Y-%m-%d %H:%M:%S\", errors=\"coerce\")\n",
    "\n",
    "#function to remove columns depending on distinct values for relevance\n",
    "def remove_col_depending_on_distinct_values(df, start_threshold=0, end_threshold=0):\n",
    "    cols_to_remove = []\n",
    "    for col in df.columns:\n",
    "        if start_threshold <= df[col].nunique() <= end_threshold:\n",
    "            cols_to_remove.append(col)\n",
    "    df.drop(columns=cols_to_remove, inplace=True)\n",
    "    return df\n",
    "\n",
    "# merge in and out time data based on EmployeeID\n",
    "time_data = pd.merge(in_time_data, out_time_data, on=\"EmployeeID\", suffixes=(\"_in\", \"_out\"))\n",
    "\n",
    "# create a new column for each day calculating the difference between out and in time in hours\n",
    "hours_columns = {}\n",
    "day_of_week_columns = {}\n",
    "for day in in_time_days.intersection(out_time_days):\n",
    "    hours_columns[f\"{day}_hours\"] = (time_data[f\"{day}_out\"] - time_data[f\"{day}_in\"]).dt.total_seconds() / 3600.0\n",
    "    day_of_week_columns[f\"{day}_day_of_week\"] = time_data[f\"{day}_in\"].dt.dayofweek\n",
    "\n",
    "# use pd.concat to avoid DataFrame fragmentation\n",
    "# Concatenate all hours columns at once and create a new column called \"duration_hours\"\n",
    "time_data = pd.concat([time_data, pd.DataFrame(hours_columns, index=time_data.index)], axis=1)\n",
    "time_data = pd.concat([time_data, pd.DataFrame(day_of_week_columns, index=time_data.index)], axis=1)\n",
    "time_data[\"duration_hours\"] = time_data[list(hours_columns.keys())].sum(axis=1)\n",
    "\n",
    "# aggregate by day of week\n",
    "day_of_week_counts = {}\n",
    "day_of_week_avg_hours = {}\n",
    "\n",
    "for i in range(7): # 0=Monday through 6=Sunday\n",
    "    count_cols = [col for col in time_data.columns if col.endswith(\"_day_of_week\")]\n",
    "    day_of_week_counts[f\"worked_on_day_{i}\"] = sum(\n",
    "        (time_data[col] == i).astype(int) for col in count_cols\n",
    "    )\n",
    "    \n",
    "    # avg hrs per day of week\n",
    "    total_hours = 0\n",
    "    for day in in_time_days.intersection(out_time_days):\n",
    "        day_col = f\"{day}_day_of_week\"\n",
    "        hours_col = f\"{day}_hours\"\n",
    "        if day_col in time_data.columns and hours_col in time_data.columns:\n",
    "            # only sum hours where the day of week matches\n",
    "            mask = time_data[day_col] == i\n",
    "            total_hours += time_data[hours_col].where(mask, 0)\n",
    "    day_of_week_avg_hours[f\"avg_hours_day_{i}\"] = total_hours / day_of_week_counts[f\"worked_on_day_{i}\"].replace(0, 1)\n",
    "\n",
    "time_data = pd.concat([time_data, pd.DataFrame(day_of_week_counts, index=time_data.index)], axis=1)\n",
    "time_data = pd.concat([time_data, pd.DataFrame(day_of_week_avg_hours, index=time_data.index)], axis=1)\n",
    "\n",
    "# remove columns with 0 distinct values\n",
    "remove_col_depending_on_distinct_values(time_data)\n",
    "\n",
    "# keep only columns: EmployeeID, duration_hours, worked_on_day_*, avg_hours_day_*\n",
    "cols_to_keep = [\"EmployeeID\", \"duration_hours\"] + [col for col in time_data.columns if col.startswith(\"worked_on_day_\") or col.startswith(\"avg_hours_day_\")]\n",
    "time_data = time_data[cols_to_keep]\n",
    "time_data = pd.concat([time_data, pd.DataFrame(hours_columns, index=time_data.index)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae868b3",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This block focuses on transforming raw arrival and departure logs into useful behavioral metrics:\n",
    "\n",
    "* **Data Cleaning & Validation:** Renames the primary key to `EmployeeID`, checks for inconsistencies between the in-time and out-time files (missing days or mismatched rows), and converts all timestamp strings into datetime objects.\n",
    "* **Daily Calculations:** Merges the datasets and calculates the daily duration of work (Departure Time - Arrival Time) and identifies the specific day of the week for each date.\n",
    "* **Aggregation:** Generates summary statistics per employee to capture work habits:\n",
    "    * `duration_hours`: Total hours worked over the recorded period.\n",
    "    * `worked_on_day_X`: Frequency of working on specific weekdays (0=Mon to 6=Sun).\n",
    "    * `avg_hours_day_X`: Average shift length for each specific weekday.\n",
    "* **Final Filtering:** Discards the raw daily timestamp columns, retaining only the `EmployeeID` and the newly calculated aggregated features for the final dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0bcc26",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e911e37",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Pipeline with proper train/test separation to avoid data leakage\n",
    "def preprocess_data_safe(dataset, fitted_scaler=None, fitted_imputers=None, fit_mode=True, \n",
    "                          encode_ordinal_cols=None, remove_from_encoding=[]):\n",
    "    \"\"\"\n",
    "    Preprocess data with proper train/test separation.\n",
    "    \n",
    "    Args:\n",
    "        dataset: DataFrame to preprocess\n",
    "        fitted_scaler: Pre-fitted StandardScaler (for test set)\n",
    "        fitted_imputers: Dict of pre-fitted imputation values (for test set)\n",
    "        fit_mode: If True, fit transformers. If False, use provided transformers\n",
    "        encode_ordinal_cols: Dict of ordinal encodings\n",
    "        remove_from_encoding: Columns to exclude from encoding\n",
    "    \n",
    "    Returns:\n",
    "        data: Preprocessed DataFrame\n",
    "        scaler: Fitted StandardScaler\n",
    "        imputers: Dict of imputation values\n",
    "    \"\"\"\n",
    "    data = dataset.copy()\n",
    "    \n",
    "    # Remove constant columns\n",
    "    constant_cols = [col for col in data.columns if data[col].nunique() <= 1]\n",
    "    if constant_cols:\n",
    "        data.drop(columns=constant_cols, inplace=True)\n",
    "    \n",
    "    # Identify column types\n",
    "    numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    categorical_cols = data.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "    \n",
    "    # Remove target and excluded columns from processing\n",
    "    if 'Attrition' in numeric_cols:\n",
    "        numeric_cols.remove('Attrition')\n",
    "    if 'EmployeeID' in numeric_cols:\n",
    "        numeric_cols.remove('EmployeeID')\n",
    "    \n",
    "    # Impute missing values\n",
    "    if fit_mode:\n",
    "        # FIT on training data\n",
    "        imputers = {}\n",
    "        if len(numeric_cols) > 0:\n",
    "            imputers['numeric'] = data[numeric_cols].median()\n",
    "        if len(categorical_cols) > 0:\n",
    "            imputers['categorical'] = data[categorical_cols].mode().iloc[0] if len(data[categorical_cols].mode()) > 0 else {}\n",
    "    else:\n",
    "        # TRANSFORM using fitted values from training\n",
    "        imputers = fitted_imputers\n",
    "    \n",
    "    # Apply imputation\n",
    "    if len(numeric_cols) > 0 and 'numeric' in imputers:\n",
    "        data[numeric_cols] = data[numeric_cols].fillna(imputers['numeric'])\n",
    "    if len(categorical_cols) > 0 and 'categorical' in imputers:\n",
    "        data[categorical_cols] = data[categorical_cols].fillna(imputers['categorical'])\n",
    "    \n",
    "    # Ordinal encoding\n",
    "    if encode_ordinal_cols:\n",
    "        for col, categories in encode_ordinal_cols.items():\n",
    "            if col in data.columns:\n",
    "                data[col] = pd.Categorical(data[col], categories=categories, ordered=True).codes\n",
    "                if col in categorical_cols:\n",
    "                    categorical_cols.remove(col)\n",
    "                if col not in numeric_cols:\n",
    "                    numeric_cols.append(col)\n",
    "    \n",
    "    # One-hot encoding\n",
    "    cols_to_encode = [col for col in categorical_cols if col not in remove_from_encoding]\n",
    "    if len(cols_to_encode) > 0:\n",
    "        data = pd.get_dummies(data, columns=cols_to_encode, drop_first=True)\n",
    "    \n",
    "    # Update numeric_cols after encoding\n",
    "    numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if 'Attrition' in numeric_cols:\n",
    "        numeric_cols.remove('Attrition')\n",
    "    if 'EmployeeID' in numeric_cols:\n",
    "        numeric_cols.remove('EmployeeID')\n",
    "    \n",
    "    # Scale numerical data\n",
    "    if fit_mode:\n",
    "        # FIT scaler on training data\n",
    "        scaler = StandardScaler()\n",
    "        if len(numeric_cols) > 0:\n",
    "            data[numeric_cols] = scaler.fit_transform(data[numeric_cols])\n",
    "    else:\n",
    "        # TRANSFORM using fitted scaler from training\n",
    "        scaler = fitted_scaler\n",
    "        if len(numeric_cols) > 0 and scaler is not None:\n",
    "            data[numeric_cols] = scaler.transform(data[numeric_cols])\n",
    "    \n",
    "    return data, scaler, imputers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43afde6",
   "metadata": {},
   "source": [
    "\n",
    "This function (`preprocess_data_safe`) prepares the raw data for machine learning while strictly preventing **data leakage** (ensuring information from the test set does not influence the training process). It operates in two modes: `fit_mode=True` (learning from training data) or `fit_mode=False` (applying learned parameters to test data).\n",
    "\n",
    "**Key steps include:**\n",
    "* **Cleaning:** Removes columns with constant values (zero variance).\n",
    "* **Imputation:** Fills missing values using the *median* for numerical columns and the *mode* for categorical columns (calculated solely on training data).\n",
    "* **Encoding:** Converts categorical variables into numbers using **Ordinal Encoding** (for ranked data) and **One-Hot Encoding** (for nominal data like Job Role).\n",
    "* **Scaling:** Standardizes numerical features (Mean=0, Std=1) using `StandardScaler` to ensure all features contribute equally to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0154753",
   "metadata": {},
   "source": [
    "## merge and split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7a5852",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Data Merging and Train/Test Split\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Merge employee and manager data first\n",
    "employee_manager_data = pd.merge(employee_data, manager_data, on=\"EmployeeID\", suffixes=(\"_emp\", \"_mgr\"))\n",
    "# Merge all datasets into a final dataset on EmployeeID\n",
    "raw_dataset = pd.merge(general_data, employee_manager_data, on=\"EmployeeID\")\n",
    "raw_dataset = pd.merge(raw_dataset, time_data, on=\"EmployeeID\")\n",
    "\n",
    "# Drop unethical columns BEFORE split\n",
    "raw_dataset.drop(columns=[\"MaritalStatus\", \"Gender\", \"Age\"], inplace=True)\n",
    "\n",
    "print(f\"Total dataset size: {len(raw_dataset)} samples\")\n",
    "print(f\"Features: {len(raw_dataset.columns)} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2c622c",
   "metadata": {},
   "source": [
    "\n",
    "This block merges all separate data sources into a single master dataset for analysis and applies ethical constraints:\n",
    "\n",
    "* **Integration:** Joins `general_data`, `employee_survey_data`, `manager_survey_data`, and the processed `time_data` into one unified DataFrame (`raw_dataset`) using `EmployeeID` as the unique key.\n",
    "* **Ethical Feature Selection:** Proactively removes sensitive demographic attributes—`MaritalStatus`, `Gender`, and `Age`—before training. This ensures the predictive model focuses on performance and work environment factors rather than personal characteristics, mitigating the risk of bias or discrimination.\n",
    "* **Verification:** Prints the final row and column counts to confirm the data integration process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01394f6a",
   "metadata": {},
   "source": [
    "## features engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f51f50",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ADVANCED FEATURE ENGINEERING\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def create_advanced_features(df):\n",
    "    \"\"\"\n",
    "    Create derived features with business logic to improve predictive power.\n",
    "    All features are created from raw data without any target leakage.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Career progression features\n",
    "    if 'YearsSinceLastPromotion' in df.columns:\n",
    "        df['YearsWithoutPromotion'] = df['YearsSinceLastPromotion']\n",
    "    \n",
    "    if 'YearsAtCompany' in df.columns and 'YearsSinceLastPromotion' in df.columns:\n",
    "        # Promotion rate: how frequently promoted relative to tenure\n",
    "        df['PromotionRate'] = df['YearsAtCompany'] / (df['YearsSinceLastPromotion'] + 1)\n",
    "    \n",
    "    if 'YearsInCurrentRole' in df.columns:\n",
    "        # Career stagnation indicator\n",
    "        df['CareerStagnation'] = (df['YearsInCurrentRole'] > 5).astype(int)\n",
    "    \n",
    "    # Compensation features\n",
    "    if 'MonthlyIncome' in df.columns and 'YearsAtCompany' in df.columns:\n",
    "        # Income per year of service\n",
    "        df['IncomePerYear'] = df['MonthlyIncome'] * 12 / (df['YearsAtCompany'] + 1)\n",
    "    \n",
    "    if 'PercentSalaryHike' in df.columns:\n",
    "        # Normalized salary growth\n",
    "        df['IncomeGrowthRate'] = df['PercentSalaryHike'] / 100\n",
    "    \n",
    "    # Work-life balance features\n",
    "    if 'duration_hours' in df.columns:\n",
    "        # Average daily working hours (assuming 260 working days per year)\n",
    "        df['AvgDailyHours'] = df['duration_hours'] / 260\n",
    "        # Overwork indicator (more than 9 hours per day)\n",
    "        df['Overwork'] = (df['AvgDailyHours'] > 9).astype(int)\n",
    "        # overtime indicator (more than 8 hours per day)\n",
    "        df['Overtime'] = (df['AvgDailyHours'] > 8).astype(int)\n",
    "    \n",
    "    # Weekend work patterns\n",
    "    weekend_cols = ['worked_on_day_5', 'worked_on_day_6']  # Saturday and Sunday\n",
    "    if all(col in df.columns for col in weekend_cols):\n",
    "        weekend_work = df['worked_on_day_5'] + df['worked_on_day_6']\n",
    "        # Weekend worker indicator (worked more than 10 weekends)\n",
    "        df['WeekendWorker'] = (weekend_work > 10).astype(int)\n",
    "    \n",
    "    # Job satisfaction composite score\n",
    "    satisfaction_cols = ['JobSatisfaction', 'EnvironmentSatisfaction', 'WorkLifeBalance']\n",
    "    if all(col in df.columns for col in satisfaction_cols):\n",
    "        # Overall satisfaction average\n",
    "        df['OverallSatisfaction'] = (\n",
    "            df['JobSatisfaction'] + \n",
    "            df['EnvironmentSatisfaction'] + \n",
    "            df['WorkLifeBalance']\n",
    "        ) / 3\n",
    "        # Low satisfaction indicator\n",
    "        df['LowSatisfaction'] = (df['OverallSatisfaction'] < 2).astype(int)\n",
    "    \n",
    "    # High-risk profile: combination of risk factors\n",
    "    if all(col in df.columns for col in ['YearsWithoutPromotion', 'JobSatisfaction', 'WorkLifeBalance']):\n",
    "        df['HighRiskProfile'] = (\n",
    "            (df['YearsWithoutPromotion'] > 3) & \n",
    "            (df['JobSatisfaction'] < 3) &\n",
    "            (df['WorkLifeBalance'] < 3)\n",
    "        ).astype(int)\n",
    "    \n",
    "    # Experience-income ratio\n",
    "    if 'TotalWorkingYears' in df.columns and 'MonthlyIncome' in df.columns:\n",
    "        df['ExperienceIncomeRatio'] = df['TotalWorkingYears'] / (df['MonthlyIncome'] / 1000 + 1)\n",
    "    \n",
    "    # Job change frequency\n",
    "    if 'NumCompaniesWorked' in df.columns and 'TotalWorkingYears' in df.columns:\n",
    "        df['JobChangeFrequency'] = df['NumCompaniesWorked'] / (df['TotalWorkingYears'] + 1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply feature engineering BEFORE split to avoid leakage\n",
    "raw_dataset = create_advanced_features(raw_dataset)\n",
    "print(f\"Features after engineering: {len(raw_dataset.columns)} columns\")\n",
    "raw_dataset.drop(columns=[\"EmployeeID\"], inplace=True)\n",
    "print(\"Advanced features created successfully\")\n",
    "\n",
    "# CRITICAL: Split BEFORE any preprocessing to avoid data leakage\n",
    "train_set, test_set = train_test_split(raw_dataset, test_size=0.2, random_state=random_state, stratify=raw_dataset['Attrition'])\n",
    "\n",
    "print(f\"Train set: {len(train_set)} samples\")\n",
    "print(f\"Test set: {len(test_set)} samples\")\n",
    "print(f\"Train class distribution: {train_set['Attrition'].value_counts().to_dict()}\")\n",
    "print(f\"Test class distribution: {test_set['Attrition'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e47172",
   "metadata": {},
   "source": [
    "\n",
    "This block enhances the dataset's predictive power by deriving new business-relevant metrics and then partitioning the data for validation.\n",
    "\n",
    "* **Feature Engineering:** The `create_advanced_features` function generates complex indicators based on HR logic, including:\n",
    "    * **Career Progression:** Metrics like `PromotionRate` and `CareerStagnation` to detect lack of growth.\n",
    "    * **Work-Life Balance:** Variables derived from time logs, such as `AvgDailyHours`, `Overtime` (working >8h), and `WeekendWorker`.\n",
    "    * **Composite Scores:** `OverallSatisfaction` combines multiple survey results, and `HighRiskProfile` flags employees showing multiple warning signs simultaneously.\n",
    "* **Data Splitting:**\n",
    "    * Removes the non-predictive `EmployeeID` column.\n",
    "    * Performs a **Stratified Train/Test Split** (80% training, 20% testing). Stratification is critical here to ensure the minority class (\"Yes\" for Attrition) is represented proportionally in both sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce30eebc",
   "metadata": {},
   "source": [
    "## preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4a2612",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Preprocessing Train and Test Sets Separately\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "ordinal_mappings = {\n",
    "    \"BusinessTravel\": [\"Non-Travel\", \"Travel_Rarely\", \"Travel_Frequently\"]\n",
    "}\n",
    "\n",
    "mutual_info_columns = [\"Department\", \"EducationField\", \"JobRole\"]\n",
    "\n",
    "# Ensure Attrition is numeric\n",
    "if train_set[target_col].dtype == \"object\":\n",
    "    train_set[target_col] = train_set[target_col].apply(lambda x: 1 if str(x).lower() in [\"yes\", \"1\"] else 0)\n",
    "if test_set[target_col].dtype == \"object\":\n",
    "    test_set[target_col] = test_set[target_col].apply(lambda x: 1 if str(x).lower() in [\"yes\", \"1\"] else 0)\n",
    "\n",
    "# Preprocess TRAIN set (fit transformers)\n",
    "train_processed, fitted_scaler, fitted_imputers = preprocess_data_safe(\n",
    "    train_set,\n",
    "    fitted_scaler=None,\n",
    "    fitted_imputers=None,\n",
    "    fit_mode=True,\n",
    "    encode_ordinal_cols=ordinal_mappings,\n",
    "    remove_from_encoding=[\"Attrition\"] + mutual_info_columns\n",
    ")\n",
    "\n",
    "print(f\"Train set preprocessed: {train_processed.shape}\")\n",
    "\n",
    "# Preprocess TEST set (use fitted transformers from train)\n",
    "test_processed, _, _ = preprocess_data_safe(\n",
    "    test_set,\n",
    "    fitted_scaler=fitted_scaler,\n",
    "    fitted_imputers=fitted_imputers,\n",
    "    fit_mode=False,\n",
    "    encode_ordinal_cols=ordinal_mappings,\n",
    "    remove_from_encoding=[\"Attrition\"] + mutual_info_columns\n",
    ")\n",
    "\n",
    "print(f\"Test set preprocessed: {test_processed.shape}\")\n",
    "print(f\"NO DATA LEAKAGE: Scaler and imputers fitted only on train set\")\n",
    "\n",
    "# Prepare X and y\n",
    "X_train_full = train_processed.drop(columns=[target_col] + [col for col in mutual_info_columns if col in train_processed.columns], errors=\"ignore\")\n",
    "X_test_full = test_processed.drop(columns=[target_col] + [col for col in mutual_info_columns if col in test_processed.columns], errors=\"ignore\")\n",
    "y_train = train_processed[target_col]\n",
    "y_test = test_processed[target_col]\n",
    "\n",
    "# Ensure X_train and X_test have same columns\n",
    "common_cols = X_train_full.columns.intersection(X_test_full.columns)\n",
    "X_train = X_train_full[common_cols]\n",
    "X_test = X_test_full[common_cols]\n",
    "\n",
    "print(f\"\\nFinal feature count: {len(common_cols)}\")\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ecc50c",
   "metadata": {},
   "source": [
    "\n",
    "This block executes the data transformation pipeline while strictly adhering to machine learning best practices:\n",
    "\n",
    "* **Target Encoding:** Converts the categorical target variable `Attrition` (\"Yes\"/\"No\") into a binary format (1/0) for algorithmic processing.\n",
    "* **Sequential Preprocessing:**\n",
    "    * **Train Set:** Runs in `fit_mode=True` to learn scaling parameters (mean, std) and imputation values from training data only.\n",
    "    * **Test Set:** Runs in `fit_mode=False` to apply those learned parameters to the test data. This guarantees that the model never \"sees\" the test distribution during training (preventing data leakage).\n",
    "* **Feature Alignment:** Finalizes the $X$ (features) and $y$ (target) matrices, ensuring that both training and testing datasets share the exact same column structure (handling any discrepancies in one-hot encoding)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d0b131",
   "metadata": {},
   "source": [
    "## feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ef0eba",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 1: Feature Selection Using ANOVA (on TRAIN set only)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Calculate ANOVA on TRAIN set only (no leakage)\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "# Use SelectKBest to get scores\n",
    "selector = SelectKBest(f_classif, k='all')\n",
    "selector.fit(X_train, y_train)\n",
    "\n",
    "# Get feature scores\n",
    "feature_scores = pd.Series(selector.scores_, index=X_train.columns)\n",
    "feature_scores = feature_scores.sort_values(ascending=False)\n",
    "\n",
    "# Filter out unwanted patterns\n",
    "exclude_patterns = [\"day_of_week\", \"avg_hours_day_\", r\"\\d{4}-\\d{2}-\\d{2}_hours\"]\n",
    "for pattern in exclude_patterns:\n",
    "    feature_scores = feature_scores[~feature_scores.index.str.contains(pattern, regex=True)]\n",
    "\n",
    "print(f\"\\nTop 20 Features by ANOVA F-score:\")\n",
    "print(feature_scores.head(20))\n",
    "\n",
    "# Select top features\n",
    "top_k = len(feature_scores) \n",
    "top_features = feature_scores.head(top_k).index.tolist()\n",
    "\n",
    "print(f\"\\nSelected {top_k} features\")\n",
    "print(f\"Features: {', '.join(top_features[:10])}...\")\n",
    "\n",
    "# Apply feature selection\n",
    "X_train_selected = X_train[top_features]\n",
    "X_test_selected = X_test[top_features]\n",
    "\n",
    "print(f\"X_train: {X_train.shape} → {X_train_selected.shape}\")\n",
    "print(f\"X_test: {X_test.shape} → {X_test_selected.shape}\")\n",
    "\n",
    "# reorder columns based on feature importance\n",
    "X_train_selected = X_train_selected[feature_scores.head(top_k).index]\n",
    "X_test_selected = X_test_selected[feature_scores.head(top_k).index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ece483",
   "metadata": {},
   "source": [
    "\n",
    "This part identifies the most relevant variables to predict employee turnover using statistical testing:\n",
    "\n",
    "* **ANOVA Analysis:** Uses the **Analysis of Variance (F-test)** to score each feature based on how strongly it discriminates between employees who stay and those who leave. Crucially, this is calculated *only* on the training set to maintain validity.\n",
    "* **Noise Filtering:** Explicitly removes granular, noisy columns (like raw daily timestamps or specific days of the week) to focus the model on the meaningful aggregated metrics created earlier (e.g., `AvgDailyHours`).\n",
    "* **Ranking:** Sorts the remaining features by importance and subsets the data, ensuring the model prioritizes the strongest predictors (like tenure, overtime, or age) while ignoring irrelevant noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1f61d8",
   "metadata": {},
   "source": [
    "## smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc7e845",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 2: Nested Cross-Validation for SMOTE Strategy\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Use nested CV for more robust SMOTE strategy selection\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "# Inner CV for hyperparameter tuning\n",
    "inner_cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=random_state)\n",
    "\n",
    "# Test SMOTE strategies with proper validation\n",
    "smote_strategies = [\n",
    "    0.40, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49,\n",
    "    0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59,\n",
    "    0.60, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69,\n",
    "    0.70, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79,\n",
    "    0.80,\n",
    "    ]\n",
    "smote_results = {}\n",
    "\n",
    "print(\"\\nTesting SMOTE strategies with nested cross-validation...\")\n",
    "for strategy in smote_strategies:\n",
    "    # Create pipeline to prevent leakage\n",
    "    pipeline = ImbPipeline([\n",
    "        ('smote', SMOTE(random_state=random_state, k_neighbors=10, sampling_strategy=strategy)),\n",
    "        ('classifier', RandomForestClassifier(n_estimators=50, random_state=random_state, n_jobs=-1))\n",
    "    ])\n",
    "    \n",
    "    # Cross-validate on training set\n",
    "    cv_scores = cross_val_score(pipeline, X_train_selected, y_train, cv=inner_cv, scoring='f1', n_jobs=-1)\n",
    "    smote_results[strategy] = cv_scores.mean()\n",
    "    \n",
    "    print(f\"  Strategy {strategy:.2f}: CV F1={cv_scores.mean():.6f} (std={cv_scores.std():.4f})\")\n",
    "\n",
    "# Select best strategy based on CV performance\n",
    "best_smote_strategy = max(smote_results, key=smote_results.get)\n",
    "best_smote_f1 = smote_results[best_smote_strategy]\n",
    "\n",
    "print(f\"\\nBest SMOTE strategy: {best_smote_strategy} (CV F1={best_smote_f1:.6f})\")\n",
    "\n",
    "# Apply SMOTE with best strategy\n",
    "smote = SMOTE(random_state=random_state, k_neighbors=10, sampling_strategy=best_smote_strategy)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train_selected, y_train)\n",
    "\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(f\"  Original: {pd.Series(y_train).value_counts().sort_index().to_dict()}\")\n",
    "print(f\"  After SMOTE: {pd.Series(y_train_balanced).value_counts().sort_index().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b16c87",
   "metadata": {},
   "source": [
    "\n",
    "In here the code addresses the significant class imbalance (far fewer employees leave than stay) using a data-driven approach rather than a fixed rule:\n",
    "\n",
    "* **Pipeline Validation:** It iterates through various sampling strategies (ratios from 0.40 to 0.80) inside a **Pipeline**. This ensures that synthetic data generation (SMOTE) occurs *only* on the training folds during cross-validation, preventing data leakage into the validation folds.\n",
    "* **Optimization:** It evaluates each strategy using the **F1-score** (which balances precision and recall) to find the optimal ratio of minority-to-majority class samples.\n",
    "* **Final Resampling:** Once the best ratio is identified, it applies SMOTE to the full training set, creating a balanced dataset (`X_train_balanced`) ready for the final model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e0ea53",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5917ac",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 3: Random Forest Model Training with Optimized Hyperparameters\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Optimize Random Forest\n",
    "print(\"\\nOptimizing Random Forest...\")\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [5, 10, 15, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "}\n",
    "\n",
    "grid_search_rf = GridSearchCV(\n",
    "    RandomForestClassifier(random_state=random_state, n_jobs=-1),\n",
    "    param_grid_rf,\n",
    "    cv=5,\n",
    "    scoring='f1',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "grid_search_rf.fit(X_train_balanced, y_train_balanced)\n",
    "print(f\"\\nBest Random Forest CV F1: {grid_search_rf.best_score_:.3f}\")\n",
    "print(\"\\nBest parameters:\")\n",
    "for param, value in grid_search_rf.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "rf_opt = grid_search_rf.best_estimator_\n",
    "\n",
    "# Cross-validate the best model\n",
    "rf_cv_scores = cross_val_score(rf_opt, X_train_balanced, y_train_balanced, cv=5, scoring='f1')\n",
    "print(f\"\\nRandom Forest CV F1: {rf_cv_scores.mean():.3f} (std={rf_cv_scores.std():.3f})\")\n",
    "\n",
    "# Store model for evaluation\n",
    "models = {\n",
    "    \"RandomForest_Optimized\": rf_opt,\n",
    "}\n",
    "\n",
    "# Store grid_search for compatibility with later code\n",
    "grid_search = grid_search_rf\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 4: Model Evaluation and Probability Calibration\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for name, m in models.items():\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"{name} - EVALUATION RESULTS\")\n",
    "    print(\"=\"*70)\n",
    "    t_start = time.time()\n",
    "    \n",
    "    # Model is already fitted, no need to refit\n",
    "    # m.fit(X_train_balanced, y_train_balanced)\n",
    "    \n",
    "    # Use F1 score for CV instead of accuracy (better for imbalanced data)\n",
    "    x_val_scores_f1 = cross_val_score(m, X_train_balanced, y_train_balanced, cv=5, scoring=\"f1\")\n",
    "    print(f\"\\nCross-validation F1 mean {x_val_scores_f1.mean():.4f}, std dev {x_val_scores_f1.std():.4f}\")\n",
    "    \n",
    "    # Probability calibration for better threshold optimization\n",
    "    print(\"\\n\" + \"-\"*70)\n",
    "    print(\"Probability Calibration (Platt scaling)\")\n",
    "    print(\"-\"*70)\n",
    "    \n",
    "    from sklearn.calibration import CalibratedClassifierCV\n",
    "    \n",
    "    # Split training data for calibration\n",
    "    X_train_cal, X_val_cal, y_train_cal, y_val_cal = train_test_split(\n",
    "        X_train_balanced, y_train_balanced,\n",
    "        test_size=0.2,\n",
    "        random_state=random_state,\n",
    "        stratify=y_train_balanced\n",
    "    )\n",
    "    \n",
    "    # Create new instance of the same model type\n",
    "    from copy import deepcopy\n",
    "    m_uncal = deepcopy(m)\n",
    "    m_uncal.fit(X_train_cal, y_train_cal)\n",
    "    \n",
    "    # Calibrate probabilities\n",
    "    calibrated_model = CalibratedClassifierCV(\n",
    "        m_uncal,\n",
    "        method='sigmoid',  # Platt scaling\n",
    "        cv='prefit'\n",
    "    )\n",
    "    calibrated_model.fit(X_val_cal, y_val_cal)\n",
    "    print(f\"Model calibrated using Platt scaling\")\n",
    "    \n",
    "    # Get predictions from calibrated model\n",
    "    y_pred = calibrated_model.predict(X_test_selected)\n",
    "    \n",
    "    print(\"Confusion Matrix:\")\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(cm)\n",
    "    \n",
    "    # Calculate key metrics for attrition detection (class 1)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    precision_class1 = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall_class1 = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1_class1 = 2 * (precision_class1 * recall_class1) / (precision_class1 + recall_class1) if (precision_class1 + recall_class1) > 0 else 0\n",
    "    \n",
    "    print(f\"\\nClass 1 (Attrition) Metrics:\")\n",
    "    print(f\"  Precision: {precision_class1:.3f} ({tp}/{tp+fp}) - How many alerts are real\")\n",
    "    print(f\"  Recall:    {recall_class1:.3f} ({tp}/{tp+fn}) - How many attritions detected\")\n",
    "    print(f\"  F1-Score:  {f1_class1:.3f} - Balance between precision/recall\")\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # Get probability scores from calibrated model\n",
    "    if hasattr(calibrated_model, 'predict_proba'):\n",
    "        y_prob = calibrated_model.predict_proba(X_test_selected)[:, 1]\n",
    "        print(f\"\\nUsing calibrated probabilities for threshold optimization\")\n",
    "    elif hasattr(calibrated_model, 'decision_function'):\n",
    "        y_prob = calibrated_model.decision_function(X_test_selected)\n",
    "        print(f\"\\nUsing calibrated decision scores for threshold optimization\")\n",
    "    else:\n",
    "        print(\"Warning: Model doesn't support probability prediction, skipping ROC-AUC\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\nROC-AUC Score: {roc_auc_score(y_test, y_prob):.4f}\")\n",
    "    fpr, tpr, thresholds_roc = roc_curve(y_test, y_prob)\n",
    "    roc_auc_val = auc(fpr, tpr)\n",
    "    \n",
    "    # STEP 5: Optimize decision threshold\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STEP 5: Optimizing Decision Threshold\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    precisions, recalls, thresholds = precision_recall_curve(y_test, y_prob)\n",
    "    f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-10)\n",
    "    \n",
    "    # Find threshold that maximizes F1\n",
    "    optimal_idx_f1 = np.argmax(f1_scores)\n",
    "    optimal_threshold_f1 = thresholds[optimal_idx_f1] if optimal_idx_f1 < len(thresholds) else 0.0\n",
    "    \n",
    "    # Find threshold with minimum precision constraint (35%)\n",
    "    min_precision_threshold = 0.35\n",
    "    valid_indices = precisions >= min_precision_threshold\n",
    "    \n",
    "    if valid_indices.any():\n",
    "        # Among valid precisions, find the one with best F1\n",
    "        valid_f1s = f1_scores[valid_indices]\n",
    "        if len(valid_f1s) > 0:\n",
    "            best_valid_idx = np.argmax(valid_f1s)\n",
    "            # Get original index in precisions array\n",
    "            original_indices = np.where(valid_indices)[0]\n",
    "            optimal_idx_constrained = original_indices[best_valid_idx]\n",
    "            optimal_threshold_constrained = thresholds[optimal_idx_constrained] if optimal_idx_constrained < len(thresholds) else 0.0\n",
    "        else:\n",
    "            optimal_threshold_constrained = optimal_threshold_f1\n",
    "    else:\n",
    "        optimal_threshold_constrained = optimal_threshold_f1\n",
    "    \n",
    "    # Use constrained threshold\n",
    "    optimal_threshold = optimal_threshold_constrained\n",
    "    \n",
    "    # Predict with optimized threshold\n",
    "    y_pred_optimized = (y_prob >= optimal_threshold).astype(int)\n",
    "    cm_opt = confusion_matrix(y_test, y_pred_optimized)\n",
    "    tn_opt, fp_opt, fn_opt, tp_opt = cm_opt.ravel()\n",
    "    precision_opt = tp_opt / (tp_opt + fp_opt) if (tp_opt + fp_opt) > 0 else 0\n",
    "    recall_opt = tp_opt / (tp_opt + fn_opt) if (tp_opt + fn_opt) > 0 else 0\n",
    "    f1_opt = 2 * (precision_opt * recall_opt) / (precision_opt + recall_opt) if (precision_opt + recall_opt) > 0 else 0\n",
    "    \n",
    "    improvement_pct = ((f1_opt - f1_class1) / f1_class1 * 100) if f1_class1 > 0 else 0\n",
    "    \n",
    "    print(f\"\\nDEFAULT Threshold (0.5):\")\n",
    "    print(f\"   Precision: {precision_class1:.3f} | Recall: {recall_class1:.3f} | F1: {f1_class1:.3f}\")\n",
    "    print(f\"   Confusion Matrix: [[{tn:3d} {fp:3d}] [{fn:3d} {tp:3d}]]\")\n",
    "    \n",
    "    print(f\"\\nOPTIMAL Threshold ({optimal_threshold:.4f}) - With Precision >= 35% constraint:\")\n",
    "    print(f\"   Precision: {precision_opt:.3f} ({tp_opt}/{tp_opt+fp_opt}) - {precision_opt*100:.1f}% of alerts are real\")\n",
    "    print(f\"   Recall:    {recall_opt:.3f} ({tp_opt}/{tp_opt+fn_opt}) - Detects {recall_opt*100:.1f}% of attritions\")\n",
    "    print(f\"   F1-Score:  {f1_opt:.3f} -> +{improvement_pct:.1f}% improvement\")\n",
    "    print(f\"   Confusion Matrix: [[{tn_opt:3d} {fp_opt:3d}] [{fn_opt:3d} {tp_opt:3d}]]\")\n",
    "    \n",
    "    print(f\"\\nBusiness Impact:\")\n",
    "    print(f\"   Detected: {tp_opt}/{tp_opt+fn_opt} attritions ({recall_opt*100:.1f}%)\")\n",
    "    print(f\"   Cost: Need to interview {tp_opt+fp_opt} employees ({fp_opt} unnecessary)\")\n",
    "    print(f\"   Efficiency: {precision_opt*100:.1f}% of interventions are useful\")\n",
    "    print(f\"   Missed: {fn_opt} attritions will leave undetected\")\n",
    "    print(f\"   Detected: {tp_opt}/{tp_opt+fn_opt} attritions ({recall_opt*100:.1f}%)\")\n",
    "    print(f\"   Cost: Need to interview {tp_opt+fp_opt} employees ({fp_opt} unnecessary)\")\n",
    "    print(f\"   Efficiency: {precision_opt*100:.1f}% of interventions are useful\")\n",
    "    print(f\"   Missed: {fn_opt} attritions will leave undetected\")\n",
    "\n",
    "    print(f\"\\nTime taken: {time.time() - t_start:.2f} seconds\")\n",
    "\n",
    "    distances = np.sqrt(fpr**2 + (1 - tpr)**2)\n",
    "    min_idx = np.argmin(distances)\n",
    "    min_distance = distances[min_idx]\n",
    "    closest_fpr = fpr[min_idx]\n",
    "    closest_tpr = tpr[min_idx]\n",
    "    \n",
    "    # Find optimal threshold point on ROC curve\n",
    "    optimal_fpr_idx = np.argmin(np.abs(thresholds_roc - optimal_threshold)) if len(thresholds_roc) > 0 else 0\n",
    "    optimal_fpr = fpr[optimal_fpr_idx] if optimal_fpr_idx < len(fpr) else 0\n",
    "    optimal_tpr = tpr[optimal_fpr_idx] if optimal_fpr_idx < len(tpr) else 1\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.plot(fpr, tpr, 'b-', linewidth=2, label=f\"ROC curve (AUC = {roc_auc_val:.4f})\")\n",
    "    plt.plot([0, 1], [0, 1], \"k--\", label=\"Random\")\n",
    "\n",
    "    # Mark closest point to (0,1)\n",
    "    plt.plot([0, closest_fpr], [1, closest_tpr], \"r-\", linewidth=2, alpha=0.5,\n",
    "            label=f\"Ideal point ({closest_fpr:.3f}, {closest_tpr:.3f})\")\n",
    "    \n",
    "    # Mark optimal threshold point\n",
    "    plt.plot(optimal_fpr, optimal_tpr, \"go\", markersize=12, \n",
    "            label=f\"Optimal threshold={optimal_threshold:.3f}\\nTPR={optimal_tpr:.3f}, FPR={optimal_fpr:.3f}\")\n",
    "\n",
    "    # Add distance annotation\n",
    "    mid_x = closest_fpr / 2\n",
    "    mid_y = (1 + closest_tpr) / 2\n",
    "    plt.text(mid_x, mid_y, f\"d = {min_distance:.4f}\",\n",
    "            fontsize=9, color=\"red\",\n",
    "            bbox=dict(boxstyle=\"round\", facecolor=\"white\", edgecolor=\"red\", alpha=0.7)\n",
    "            )\n",
    "\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.xlabel(\"False Positive Rate\", fontsize=12)\n",
    "    plt.ylabel(\"True Positive Rate\", fontsize=12)\n",
    "    plt.title(f\"{name} - ROC Curve\\nF1={f1_opt:.3f}, Precision={precision_opt:.3f}, Recall={recall_opt:.3f}\", fontsize=14)\n",
    "    plt.legend(loc=\"lower right\", fontsize=10)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"OPTIMIZATION COMPLETE!\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1070a782",
   "metadata": {},
   "source": [
    "\n",
    "This comprehensive block handles the entire modeling lifecycle, shifting from raw prediction to actionable business insights:\n",
    "\n",
    "* **Hyperparameter Tuning (Grid Search):** Instead of guessing settings, it exhaustively tests different combinations of Random Forest parameters (like tree depth and number of trees) to find the configuration that yields the highest F1-score on the balanced data.\n",
    "* **Probability Calibration (Platt Scaling):** Standard models can be \"overconfident.\" This step recalibrates the model's probability outputs so that a predicted \"70% risk of leaving\" actually corresponds to a 70% real-world probability.\n",
    "* **Strategic Threshold Optimization:** This is critical. The default threshold of 0.5 is rarely optimal for HR.\n",
    "    * The code dynamically finds a **custom threshold** that maximizes the F1-score.\n",
    "    * **Constraint:** It enforces a *minimum precision of 35%*. This ensures that while catching potential leavers (Recall), the HR team doesn't waste too much time interviewing employees who are actually happy (False Positives).\n",
    "* **Business Impact Analysis:** Translates abstract metrics into HR terms:\n",
    "    * **Efficiency:** Percentage of interventions that are useful.\n",
    "    * **Cost:** Number of unnecessary interviews conducted.\n",
    "    * **Risk:** Number of resignations the model failed to predict.\n",
    "* **Visualization:** Plots the **ROC Curve** to visually assess the trade-off between catching leavers and raising false alarms, marking the optimal operating point."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
