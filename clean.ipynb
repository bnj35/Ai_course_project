{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "daf27323",
   "metadata": {},
   "source": [
    "# Clean Code\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f159546d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import tarfile\n",
    "import os\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5036adc2",
   "metadata": {},
   "source": [
    "## load files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09832131",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'data/'\n",
    "employee_file_name = 'employee_survey_data.csv'\n",
    "general_file_name = 'general_data.csv'\n",
    "manager_file_name = 'manager_survey_data.csv'\n",
    "#for the in out time \n",
    "time_folder_path = 'in_out_time/'\n",
    "in_time_file_name = 'in_time.csv'\n",
    "out_time_file_name = 'out_time.csv'\n",
    "\n",
    "# Load each dataset\n",
    "employee_data = pd.read_csv(os.path.join(folder_path, employee_file_name))\n",
    "general_data = pd.read_csv(os.path.join(folder_path, general_file_name))\n",
    "manager_data = pd.read_csv(os.path.join(folder_path, manager_file_name))\n",
    "in_time_data = pd.read_csv(os.path.join(folder_path, time_folder_path, in_time_file_name))\n",
    "out_time_data = pd.read_csv(os.path.join(folder_path, time_folder_path, out_time_file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8912facb",
   "metadata": {},
   "source": [
    "## Merge In/Out time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55759f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Days missing in either in_time or out_time data: set()\n"
     ]
    }
   ],
   "source": [
    "# merge in_time and out_time data on the first column (Unknown that is actually EmployeeID)\n",
    "# rename the first column to EmployeeID for both datasets because it is unnamed\n",
    "in_time_data.rename(columns={in_time_data.columns[0]: 'EmployeeID'}, inplace=True)\n",
    "out_time_data.rename(columns={out_time_data.columns[0]: 'EmployeeID'}, inplace=True)\n",
    "\n",
    "#check if days are present in both datasets\n",
    "in_time_days = set(in_time_data.columns[1:])\n",
    "out_time_days = set(out_time_data.columns[1:])\n",
    "missing_in_out = in_time_days.difference(out_time_days)\n",
    "# display the missing days\n",
    "print(f\"Days missing in either in_time or out_time data: {missing_in_out}\")\n",
    "\n",
    "# go through each column to check empty cells present only in one of the datasets\n",
    "for day in in_time_days.intersection(out_time_days):\n",
    "    in_time_empty = set(in_time_data.index[in_time_data[day].isnull()])\n",
    "    out_time_empty = set(out_time_data.index[out_time_data[day].isnull()])\n",
    "    missing_in_out_rows = in_time_empty.symmetric_difference(out_time_empty)\n",
    "    if missing_in_out_rows:\n",
    "        print(f\"Day {day} has missing entries in either in_time or out_time data at rows: {missing_in_out_rows}\")\n",
    "\n",
    "# convert all columns except the first one to datetime format\n",
    "for col in in_time_data.columns[1:]:\n",
    "    in_time_data[col] = pd.to_datetime(in_time_data[col], format='%Y-%m-%d %H:%M:%S', errors='coerce')\n",
    "for col in out_time_data.columns[1:]:\n",
    "    out_time_data[col] = pd.to_datetime(out_time_data[col], format='%Y-%m-%d %H:%M:%S', errors='coerce')\n",
    "\n",
    "#function to remove columns depending on distinct values for relevance\n",
    "def remove_col_depending_on_distinct_values(df, start_threshold=0, end_threshold=0):\n",
    "    cols_to_remove = []\n",
    "    for col in df.columns:\n",
    "        if start_threshold <= df[col].nunique() <= end_threshold:\n",
    "            cols_to_remove.append(col)\n",
    "    df.drop(columns=cols_to_remove, inplace=True)\n",
    "    return df\n",
    "\n",
    "# merge in and out time data based on EmployeeID\n",
    "time_data = pd.merge(in_time_data, out_time_data, on='EmployeeID', suffixes=('_in', '_out'))\n",
    "\n",
    "# create a new column for each day calculating the difference between out and in time in hours\n",
    "hours_columns = {}\n",
    "day_of_week_columns = {}\n",
    "for day in in_time_days.intersection(out_time_days):\n",
    "    hours_columns[f'{day}_hours'] = (time_data[f'{day}_out'] - time_data[f'{day}_in']).dt.total_seconds() / 3600.0\n",
    "    day_of_week_columns[f'{day}_day_of_week'] = time_data[f'{day}_in'].dt.dayofweek\n",
    "\n",
    "# use pd.concat to avoid DataFrame fragmentation\n",
    "# Concatenate all hours columns at once and create a new column called \"duration_hours\"\n",
    "time_data = pd.concat([time_data, pd.DataFrame(hours_columns, index=time_data.index)], axis=1)\n",
    "time_data = pd.concat([time_data, pd.DataFrame(day_of_week_columns, index=time_data.index)], axis=1)\n",
    "time_data['duration_hours'] = time_data[list(hours_columns.keys())].sum(axis=1)\n",
    "\n",
    "# aggregate by day of week\n",
    "day_of_week_counts = {}\n",
    "day_of_week_avg_hours = {}\n",
    "\n",
    "for i in range(7): # 0=Monday through 6=Sunday\n",
    "    count_cols = [col for col in time_data.columns if col.endswith('_day_of_week')]\n",
    "    day_of_week_counts[f'worked_on_day_{i}'] = sum(\n",
    "        (time_data[col] == i).astype(int) for col in count_cols\n",
    "    )\n",
    "    \n",
    "    # avg hrs per day of week\n",
    "    total_hours = 0\n",
    "    for day in in_time_days.intersection(out_time_days):\n",
    "        day_col = f'{day}_day_of_week'\n",
    "        hours_col = f'{day}_hours'\n",
    "        if day_col in time_data.columns and hours_col in time_data.columns:\n",
    "            # only sum hours where the day of week matches\n",
    "            mask = time_data[day_col] == i\n",
    "            total_hours += time_data[hours_col].where(mask, 0)\n",
    "    day_of_week_avg_hours[f'avg_hours_day_{i}'] = total_hours / day_of_week_counts[f'worked_on_day_{i}'].replace(0, 1)\n",
    "\n",
    "time_data = pd.concat([time_data, pd.DataFrame(day_of_week_counts, index=time_data.index)], axis=1)\n",
    "time_data = pd.concat([time_data, pd.DataFrame(day_of_week_avg_hours, index=time_data.index)], axis=1)\n",
    "\n",
    "# remove columns with 0 distinct values\n",
    "remove_col_depending_on_distinct_values(time_data)\n",
    "\n",
    "# keep only columns: EmployeeID, duration_hours, worked_on_day_*, avg_hours_day_*\n",
    "cols_to_keep = ['EmployeeID', 'duration_hours'] + [col for col in time_data.columns if col.startswith('worked_on_day_') or col.startswith('avg_hours_day_')]\n",
    "time_data = time_data[cols_to_keep]\n",
    "time_data = pd.concat([time_data, pd.DataFrame(hours_columns, index=time_data.index)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b9d24b",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74769a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline with all steps included above as parameters for easy reuse\n",
    "def preprocess_data(dataset, impute_values=True, numeric_cols=None, categorical_cols=None, scale_data=True, encode_ordinal_cols=None, encode_onehot_cols=True, remove_constant_cols=True, remove_from_encoding=[]):\n",
    "    #copy the dataset to avoid modifying the original data\n",
    "    data = dataset.copy()\n",
    "    # remove constant columns\n",
    "    if remove_constant_cols:\n",
    "        data = remove_col_depending_on_distinct_values(data, end_threshold=1)\n",
    "    # identify numerical and categorical columns if not provided\n",
    "    if numeric_cols is None:\n",
    "        numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if categorical_cols is None:\n",
    "        categorical_cols = data.select_dtypes(include=['object']).columns.tolist()\n",
    "    # impute missing values\n",
    "    if impute_values:\n",
    "        if len(numeric_cols) > 0:\n",
    "            data[numeric_cols] = data[numeric_cols].fillna(data[numeric_cols].median())\n",
    "        if len(categorical_cols) > 0:\n",
    "            data[categorical_cols] = data[categorical_cols].fillna(data[categorical_cols].mode().iloc[0])\n",
    "    # ordinal encoding\n",
    "    if encode_ordinal_cols and len(categorical_cols) > 0:\n",
    "        for col, categories in encode_ordinal_cols.items():\n",
    "            if col in data.columns:\n",
    "                data[col] = pd.Categorical(data[col], categories=categories, ordered=True).codes\n",
    "                # Remove ordinally encoded columns from categorical_cols to avoid one-hot encoding them\n",
    "                if col in categorical_cols:\n",
    "                    categorical_cols.remove(col)\n",
    "                # Add to numeric_cols since it's now numeric\n",
    "                if col not in numeric_cols:\n",
    "                    numeric_cols.append(col)\n",
    "    # one-hot encoding\n",
    "    if encode_onehot_cols:\n",
    "        # If encode_onehot_cols is True, use the remaining categorical columns\n",
    "        if encode_onehot_cols is True:\n",
    "            cols_to_encode = categorical_cols\n",
    "        else:\n",
    "            cols_to_encode = encode_onehot_cols\n",
    "        \n",
    "        # Remove columns specified in remove_from_encoding\n",
    "        cols_to_encode = [col for col in cols_to_encode if col not in remove_from_encoding]\n",
    "        \n",
    "        if len(cols_to_encode) > 0:\n",
    "            data = pd.get_dummies(data, columns=cols_to_encode, drop_first=True)\n",
    "    # scale numerical data\n",
    "    if scale_data:\n",
    "        scaler = StandardScaler(with_mean=True)\n",
    "        data[numeric_cols] = scaler.fit_transform(data[numeric_cols])\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533e5b43",
   "metadata": {},
   "source": [
    "## Merge all dataset into One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "279d15b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge employee and manager data first\n",
    "employee_manager_data = pd.merge(employee_data, manager_data, on='EmployeeID', suffixes=('_emp', '_mgr'))\n",
    "# merge all datasets into a final dataset on EmployeeID\n",
    "final_dataset = pd.merge(general_data, employee_manager_data, on='EmployeeID')\n",
    "final_dataset = pd.merge(final_dataset, time_data, on='EmployeeID')\n",
    "\n",
    "# split dataset into training and testing sets\n",
    "train_set, test_set = train_test_split(final_dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# place the EmployeeID column at the front\n",
    "cols = final_dataset.columns.tolist()\n",
    "cols.insert(0, cols.pop(cols.index('EmployeeID')))\n",
    "final_dataset = final_dataset[cols]\n",
    "\n",
    "# print to verify \n",
    "\n",
    "# print(final_dataset.info())\n",
    "# print(final_dataset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15f33fa",
   "metadata": {},
   "source": [
    "## Clean final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2eb6fa18",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset = preprocess_data(final_dataset,\n",
    "                                impute_values=True,\n",
    "                                scale_data=True,\n",
    "                                encode_onehot_cols=True,\n",
    "                                remove_constant_cols=True,\n",
    "                                remove_from_encoding=['Attrition']\n",
    "                                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4b987e",
   "metadata": {},
   "source": [
    "## Correlation verification and sort colums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb431a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 7 correlated features with Attrition:\n",
      "duration_hours             0.198890\n",
      "TotalWorkingYears          0.170162\n",
      "Age                        0.159205\n",
      "YearsWithCurrManager       0.156199\n",
      "YearsAtCompany             0.134392\n",
      "JobSatisfaction            0.103068\n",
      "EnvironmentSatisfaction    0.101625\n",
      "dtype: float64\n",
      "\n",
      "Day-of-week correlations with Attrition:\n",
      "avg_hours_day_0    0.202241\n",
      "avg_hours_day_4    0.201658\n",
      "avg_hours_day_3    0.201634\n",
      "avg_hours_day_1    0.201334\n",
      "avg_hours_day_2    0.201156\n",
      "worked_on_day_4    0.045721\n",
      "worked_on_day_2    0.031411\n",
      "worked_on_day_1    0.024872\n",
      "worked_on_day_3    0.020845\n",
      "worked_on_day_0    0.013073\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_selection import f_classif, mutual_info_classif\n",
    "\n",
    "def correlation_anova_mutual_info_with_target(data, target_column, exclude_patterns=None):\n",
    "    \"\"\"\n",
    "    Calculate feature importance using correlation (for numeric features), \n",
    "    ANOVA F-statistic (for categorical features), or Mutual Information.\n",
    "    \n",
    "    Returns scores for each feature based on their relationship with the target.\n",
    "    \"\"\"\n",
    "    # Make a copy to avoid modifying original data\n",
    "    data_copy = data.copy()\n",
    "    \n",
    "    # Check if target is categorical or numeric\n",
    "    target_is_categorical = data_copy[target_column].dtype == 'object' or data_copy[target_column].nunique() <= 10\n",
    "    \n",
    "    if target_is_categorical:\n",
    "        # Encode categorical target for classification methods\n",
    "        if data_copy[target_column].dtype == 'object':\n",
    "            unique_vals = data_copy[target_column].unique()\n",
    "            if set(unique_vals).issubset({'Yes', 'No', np.nan}):\n",
    "                data_copy[target_column] = data_copy[target_column].map({'Yes': 1, 'No': 0})\n",
    "            else:\n",
    "                data_copy[target_column] = pd.Categorical(data_copy[target_column]).codes\n",
    "        \n",
    "        y = data_copy[target_column]\n",
    "        scores = {}\n",
    "        \n",
    "        # Get numeric and categorical columns\n",
    "        numeric_cols = data_copy.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        numeric_cols = [col for col in numeric_cols if col != target_column and data_copy[col].nunique(dropna=True) > 1]\n",
    "        \n",
    "        categorical_cols = data_copy.select_dtypes(include=['object']).columns.tolist()\n",
    "        \n",
    "        # For numeric features: use correlation or ANOVA F-statistic\n",
    "        for col in numeric_cols:\n",
    "            # Use absolute correlation as score\n",
    "            correlation = data_copy[[col, target_column]].corr().iloc[0, 1]\n",
    "            scores[col] = abs(correlation)\n",
    "        \n",
    "        # For categorical features: use ANOVA or Mutual Information\n",
    "        for col in categorical_cols:\n",
    "            # Encode categorical feature\n",
    "            encoded_col = pd.Categorical(data_copy[col]).codes\n",
    "            # Use mutual information for categorical vs categorical\n",
    "            mi_score = mutual_info_classif(encoded_col.values.reshape(-1, 1), y, random_state=42)[0]\n",
    "            scores[col] = mi_score\n",
    "    \n",
    "    else:\n",
    "        # Target is numeric - use correlation for all numeric features\n",
    "        numeric_data = data_copy.select_dtypes(include=[np.number])\n",
    "        const_cols = [col for col in numeric_data.columns if col != target_column and numeric_data[col].nunique(dropna=True) <= 1]\n",
    "        if const_cols:\n",
    "            numeric_data = numeric_data.drop(columns=const_cols)\n",
    "        \n",
    "        correlation = numeric_data.corr()[target_column].abs()\n",
    "        scores = correlation.to_dict()\n",
    "        del scores[target_column]  # Remove target from scores\n",
    "    \n",
    "    # Convert to Series and sort\n",
    "    scores_series = pd.Series(scores).sort_values(ascending=False)\n",
    "    \n",
    "    # Filter by patterns\n",
    "    if exclude_patterns:\n",
    "        filtered_scores = scores_series[~scores_series.index.str.contains('|'.join(exclude_patterns), regex=True)]\n",
    "    else:\n",
    "        filtered_scores = scores_series\n",
    "    \n",
    "    ordered_cols = scores_series.index.tolist()\n",
    "    return ordered_cols, scores_series, filtered_scores\n",
    "\n",
    "\n",
    "\n",
    "# Correlation verification between a target feature and others\n",
    "def correlation_with_target(data, target_column, exclude_patterns=None):\n",
    "    # Make a copy to avoid modifying original data\n",
    "    data_copy = data.copy()\n",
    "    \n",
    "    # If target column is not numeric, try to encode it\n",
    "    if target_column in data_copy.columns and data_copy[target_column].dtype == 'object':\n",
    "        # Map Yes/No to 1/0, or use label encoding for other categorical values\n",
    "        unique_vals = data_copy[target_column].unique()\n",
    "        if set(unique_vals).issubset({'Yes', 'No', np.nan}):\n",
    "            data_copy[target_column] = data_copy[target_column].map({'Yes': 1, 'No': 0})\n",
    "        else:\n",
    "            # For other categorical values, use numeric encoding\n",
    "            data_copy[target_column] = pd.Categorical(data_copy[target_column]).codes\n",
    "    \n",
    "    # Select only numeric columns for correlation\n",
    "    numeric_data = data_copy.select_dtypes(include=[np.number])\n",
    "    \n",
    "    # remove constant numeric or NaN\n",
    "    const_cols = [col for col in numeric_data.columns if col != target_column and numeric_data[col].nunique(dropna=True) <= 1]\n",
    "    if const_cols:\n",
    "        numeric_data = numeric_data.drop(columns=const_cols)\n",
    "    \n",
    "    if target_column not in numeric_data.columns:\n",
    "        raise ValueError(f\"Target column '{target_column}' could not be converted to numeric or does not exist\")\n",
    "    \n",
    "    correlation = numeric_data.corr()[target_column].sort_values(ascending=False)\n",
    "    \n",
    "    if exclude_patterns:\n",
    "        filtered_correlation = correlation[~correlation.index.str.contains('|'.join(exclude_patterns), regex=True)]\n",
    "    else:\n",
    "        filtered_correlation = correlation\n",
    "    \n",
    "    corelation_ordered = correlation.index.tolist()\n",
    "    return corelation_ordered, correlation, filtered_correlation\n",
    "\n",
    "# ordered_cols, corr_values, filtered_corr = correlation_with_target(\n",
    "#     final_dataset, \n",
    "#     \"Attrition\",\n",
    "#     exclude_patterns=[\"Attrition\", r'\\d{4}-\\d{2}-\\d{2}_hours', r'avg_hours_day_\\d+', r'worked_on_day_\\d+']\n",
    "# )\n",
    "\n",
    "ordered_cols, corr_values, filtered_corr = correlation_anova_mutual_info_with_target(\n",
    "    final_dataset,\n",
    "    \"Attrition\",\n",
    "    exclude_patterns=[\"Attrition\", r'\\d{4}-\\d{2}-\\d{2}_hours', r'avg_hours_day_\\d+', r'worked_on_day_\\d+']\n",
    ")\n",
    "\n",
    "\n",
    "#########\n",
    "# Display\n",
    "#########\n",
    "\n",
    "# positive correlation\n",
    "positive_corr = filtered_corr[filtered_corr > 0]\n",
    "# negative correlation\n",
    "negative_corr = filtered_corr[filtered_corr < 0]\n",
    "\n",
    "def order_correlation(corr_series , ascending=False):\n",
    "    absolute_corr = corr_series.abs()\n",
    "    ordered_corr = absolute_corr.sort_values(ascending=ascending)\n",
    "    return ordered_corr\n",
    "\n",
    "\n",
    "positive_ordered_corr = order_correlation(positive_corr, ascending=False)\n",
    "negative_ordered_corr = order_correlation(negative_corr, ascending=False)\n",
    "# print(f\"\\nTop 10 positively correlated features with Attrition:\\n{positive_ordered_corr}\")\n",
    "# print(f\"\\nTop 10 negatively correlated features with Attrition:\\n{negative_ordered_corr}\")\n",
    "\n",
    "absolute_filtered_corr = filtered_corr.abs()\n",
    "# ordered correlation by absolute value\n",
    "absolute_filtered_corr = absolute_filtered_corr.sort_values(ascending=False)\n",
    "\n",
    "top_n_correlated = absolute_filtered_corr.where(absolute_filtered_corr > absolute_filtered_corr.mean(), None).dropna()\n",
    "print(f\"\\nTop {len(top_n_correlated)} correlated features with Attrition:\\n{top_n_correlated}\")\n",
    "\n",
    "day_of_week_corr = corr_values[[col for col in corr_values.index if 'day' in col and ('worked_on' in col or 'avg_hours' in col)]]\n",
    "if len(day_of_week_corr) > 0:\n",
    "    print(f\"\\nDay-of-week correlations with Attrition:\\n{day_of_week_corr}\")\n",
    "    \n",
    "\n",
    "final_dataset = final_dataset[ordered_cols]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf85e9de",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c746cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reduced dataset shape: (4410, 7)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     12\u001b[39m y = final_dataset[\u001b[33m'\u001b[39m\u001b[33mAttrition\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mAttrition\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m final_dataset.columns \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mReduced dataset shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreduced_dataset.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mY:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43my\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mReduced dataset info:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mreduced_dataset.info()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "# choose only features above the mean correlation\n",
    "mean_corr = absolute_filtered_corr.mean()\n",
    "selected_features = absolute_filtered_corr[absolute_filtered_corr > mean_corr].index.tolist()\n",
    "\n",
    "# create reduced dataset with selected features\n",
    "reduced_dataset = final_dataset[selected_features]\n",
    "\n",
    "# separate features and target variable\n",
    "X = reduced_dataset.drop(columns=['Attrition'], errors='ignore')\n",
    "\n",
    "# separate target variable\n",
    "y = final_dataset['Attrition'] if 'Attrition' in final_dataset.columns else None\n",
    "\n",
    "\n",
    "print(f\"\\nReduced dataset shape: {reduced_dataset.shape}\")\n",
    "print(f\"\\nY:\\n{y.shape}\")\n",
    "\n",
    "print(f\"\\nReduced dataset info:\\n{reduced_dataset.info()}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
